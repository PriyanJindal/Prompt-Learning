# -*- coding: utf-8 -*-
"""prompt_learning_cookbook_AX.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IhN8oZJOxkft9eLY8pJte5nWdFlbl0C4

[![My Image](data:image/png;base64,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)](https://arize.com)

# Optimizing JSON Webpage Prompts with the Arize Prompt Learning SDK

In this cookbook, we demonstrate a use case of the Arize Prompt Learning SDK by optimizing a system prompt for GPT-4.1. The goal is to improve the model‚Äôs ability to generate accurate JSON representations of webpages in response to user queries. The dataset consists of prompts asking GPT to generate webpages, and we define 10 specific rules that the JSON outputs must satisfy. Using the SDK, we iteratively refine the prompt to achieve high accuracy on the training set, and then evaluate its performance on a separate test set.
"""
import pandas as pd
from phoenix.evals import llm_generate
from phoenix.evals import OpenAIModel
import os
import getpass
import re
import json
import nest_asyncio
import openai
from arize_toolkit.extensions.prompt_optimizer import PromptLearningOptimizer
from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score
import requests
import urllib.parse
from pathlib import Path
#!pip install arize-phoenix-evals arize-phoenix-client tiktoken openai arize-toolkit

# CONFIG: Experiment settings - adjust as needed
NUM_SAMPLES = 100  # Number of rows to sample from the full dataset, 0 for all
TRAIN_SPLIT_FRACTION = 0.5  # Fraction of data to use for training (rest for testing)


# EXPERIMENT CONFIGURATION
NUM_OPTIMIZATION_LOOPS = 1  # Number of optimization loops per experiment (used by simple_test and optimize_loop)

# CONFIG: Skip tasks with all failed generations (final_accuracy == 0.0)
SKIP_ZERO_ACCURACY_TASKS = True  # Set to False to include all tasks, even if all generations failed

# USAGE EXAMPLES:
# 1. Run BigBench Hard experiments:
#    - Call run_bbh_experiments() to run all tasks
#    - Results saved to "bbh_results.csv" and "bbh_ground_truth_comparison.csv"
#
# 2. Run single task:
#    - Load data: dataset, train_set, test_set, train_targets, test_targets = data_prep_json("bbh-download/task.json")
#    - Run experiment: results, results_df = simple_test(train_set, test_set, prompt, evaluator, results_df)
#    - Compare with ground truth: comparison = compare_results_with_targets(results, test_targets, task_type="boolean")
#
# 3. Control optimization loops:
#    - Set NUM_OPTIMIZATION_LOOPS to control how many iterations per experiment

import nest_asyncio, re
nest_asyncio.apply()

# 1Ô∏è‚É£  stricter variable detector
from phoenix.evals.templates import PromptTemplate, PromptPartTemplate
_TEMPLATE_RE = re.compile(r"\{([a-zA-Z_][a-zA-Z0-9_]*)\}")
def _parse_variables_strict(self, tmpl: list[PromptPartTemplate]):  # [...]
    vars = set()
    for p in tmpl:
        vars.update(m.group(1) for m in _TEMPLATE_RE.finditer(p.template))
    return list(vars)
PromptTemplate._parse_variables = _parse_variables_strict

# 2Ô∏è‚É£  literal‚Äëbrace formatter
from phoenix.evals.templates import PromptPart, MultimodalPrompt
def _format_literal(self, variable_values, options=None):  # [...]
    prompt_msgs = []
    for part in self.prompt(options):
        msg = part.template
        for var in self.variables:
            msg = msg.replace(f"{{{var}}}", str(variable_values[var]))
        prompt_msgs.append(PromptPart(content_type=part.content_type, content=msg))
    return MultimodalPrompt(parts=prompt_msgs)
PromptTemplate.format = _format_literal

"""## OpenAI Key
We will be using OpenAI to generate the webpage jsons.
"""

client = openai.Client(api_key=os.getenv("OPENAI_API_KEY"))

"""## BigBench Hard JSON Data Loading

Functions for downloading and processing BigBench Hard JSON datasets.
"""
def download_bbh_json_files(download_dir="bbh-download"):
    """
    Download all JSON files from BigBench Hard repository.
    
    Args:
        download_dir (str): Directory to download files to
        
    Returns:
        list: List of downloaded file paths
    """
    os.makedirs(download_dir, exist_ok=True)
    
    # Known BigBench Hard tasks from the literature
    bbh_tasks = [
        "boolean_expressions",
        "causal_judgement", 
        "date_understanding",
        "disambiguation_qa",
        "dyck_languages",
        "formal_fallacies",
        "geometric_shapes",
        "hyperbaton",
        "logical_deduction_five_objects",
        "logical_deduction_seven_objects",
        "logical_deduction_three_objects",
        "movie_recommendation",
        "multistep_arithmetic_two",
        "navigate",
        "object_counting",
        "penguins_in_a_table",
        "reasoning_about_colored_objects",
        "ruin_names",
        "salient_translation_error_detection",
        "snarks",
        "sports_understanding",
        "temporal_sequences",
        "tracking_shuffled_objects_five_objects",
        "tracking_shuffled_objects_seven_objects",
        "tracking_shuffled_objects_three_objects",
        "web_of_lies",
        "word_sorting"
    ]
    
    base_url = "https://raw.githubusercontent.com/suzgunmirac/BIG-Bench-Hard/main/bbh"
    downloaded_files = []
    
    for task_name in bbh_tasks:
        file_url = f"{base_url}/{task_name}.json"
        local_path = os.path.join(download_dir, f"{task_name}.json")
        
        try:
            print(f"Downloading {task_name}.json...")
            response = requests.get(file_url)
            response.raise_for_status()
            
            with open(local_path, 'w', encoding='utf-8') as f:
                f.write(response.text)
            
            downloaded_files.append(local_path)
            print(f"‚úì Downloaded {task_name}.json")
            
        except requests.exceptions.RequestException as e:
            print(f"‚úó Failed to download {task_name}.json: {e}")
            # Try alternative URL structure
            try:
                alt_url = f"https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/{task_name}/task.json"
                response = requests.get(alt_url)
                response.raise_for_status()
                
                with open(local_path, 'w', encoding='utf-8') as f:
                    f.write(response.text)
                
                downloaded_files.append(local_path)
                print(f"‚úì Downloaded {task_name}.json from alternative source")
                
            except requests.exceptions.RequestException as e2:
                print(f"‚úó Failed to download {task_name}.json from alternative source: {e2}")
    
    print(f"\nDownloaded {len(downloaded_files)} JSON files to {download_dir}/")
    return downloaded_files

def load_json_to_dataframe(json_file_path):
    """
    Load a BigBench Hard JSON file and convert to DataFrame.
    
    Args:
        json_file_path (str): Path to the JSON file
        
    Returns:
        pandas.DataFrame: DataFrame with 'input' and 'target' columns
    """
    with open(json_file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # Handle different JSON structures
    if 'examples' in data:
        # Standard BIG-bench format
        examples = data['examples']
        df_data = []
        
        for example in examples:
            input_text = example.get('input', '')
            target = example.get('target', '')
            
            # Handle different target formats
            if isinstance(target, list):
                target = target[0] if target else ''
            elif isinstance(target, dict):
                # Handle target_scores format
                if 'target_scores' in example:
                    target_scores = example['target_scores']
                    # Find the target with highest score
                    target = max(target_scores.items(), key=lambda x: x[1])[0]
                else:
                    target = str(target)
            
            df_data.append({
                'input': input_text,
                'target': str(target)
            })
        
        return pd.DataFrame(df_data)
    
    elif isinstance(data, list):
        # Direct list of examples
        df_data = []
        for example in data:
            input_text = example.get('input', '')
            target = example.get('target', example.get('output', ''))
            
            if isinstance(target, list):
                target = target[0] if target else ''
            
            df_data.append({
                'input': input_text,
                'target': str(target)
            })
        
        return pd.DataFrame(df_data)
    
    else:
        raise ValueError(f"Unknown JSON structure in {json_file_path}")

def data_prep_json(json_file_path, num_samples=None):
    """
    Prepare training and test datasets from JSON file.
    
    Args:
        json_file_path (str): Path to the JSON file
        num_samples (int): Number of samples to use (None for all)
        
    Returns:
        tuple: (full_dataset, train_set, test_set, train_targets, test_targets)
    """
    if num_samples is None:
        num_samples = NUM_SAMPLES
    
    # Load JSON data
    dataset_full = load_json_to_dataframe(json_file_path)
    
    # Sample if requested
    if num_samples > 0 and len(dataset_full) > num_samples:
        dataset_sample = dataset_full.sample(num_samples, random_state=42)
    else:
        dataset_sample = dataset_full
    
    # Split into train and test
    train_set_with_targets = dataset_sample.sample(frac=TRAIN_SPLIT_FRACTION, random_state=42)
    test_set_with_targets = dataset_sample.drop(train_set_with_targets.index)
    
    # Save targets separately
    train_targets = train_set_with_targets['target'].copy()
    test_targets = test_set_with_targets['target'].copy()
    
    # Remove target column from datasets
    train_set = train_set_with_targets.drop('target', axis=1).copy()
    test_set = test_set_with_targets.drop('target', axis=1).copy()
    

    
    return dataset_sample, train_set, test_set, train_targets, test_targets

def get_available_bbh_tasks(download_dir="bbh-download"):
    """
    Get list of available BigBench Hard tasks from downloaded files.
    
    Args:
        download_dir (str): Directory containing downloaded JSON files
        
    Returns:
        list: List of task names (without .json extension)
    """
    if not os.path.exists(download_dir):
        return []
    
    tasks = []
    for filename in os.listdir(download_dir):
        if filename.endswith('.json'):
            tasks.append(filename[:-5])  # Remove .json extension
    
    return sorted(tasks)



"""## Initial System Prompt

Initialize your system prompt. This is the original prompt that will be tested and optimized.
"""

#system_prompt = "You are an expert in JSON webpage creation. This is your task: {input}"
#system_prompt = "You are an expert in solving boolean expressions. Return your answer **in JSON** with a single key `result` whose value is either \"True\" or \"False\". This is your task: {input}"
#system_prompt = "You are an expert in solving truthfulness puzzles. Return your answer **in JSON** with a single key `result` whose value is either \"Yes\" or \"No\". This is your task: {input}"

"""## Evaluator

Here we initialize our evaluator. This uses LLM as a Judge, or using an LLM to evaluate our outputs.  

We will pass in a set of 10 rules to this LLM. It will evaluate each generated JSON against these 10 rules, checking if all are satisfied.

Accordingly, it will give a correctness label, either correct or incorrect.

Additionally, it will attach an explanation as to why it chose correct or incorrect. These explanations will be used to optimize the prompt.
"""

nest_asyncio.apply()

def find_correctness(output):
    """Extract correctness from LLM output"""
    # Look for "correct" or "incorrect" in the response
    pattern = r'"correctness":\s*"?(correct|incorrect)"?'
    match = re.search(pattern, output, re.IGNORECASE)
    if match:
        return match.group(1).lower()
    else:
        return None

def find_explanation(output):
    """Extract explanation from LLM output"""
    # Look for explanation field in JSON
    pattern = r'"explanation":\s*"([^"]*)"'
    match = re.search(pattern, output, re.IGNORECASE)
    if match:
        return match.group(1)
    else:
        return None

def evaluate_output_parser(response: str, row_index: int) -> dict:
    """Parser function for evaluate_output evaluator"""
    try:
        correctness = find_correctness(response)
        explanation = find_explanation(response)
        
        # Debug: Log parsing issues
        if correctness is None:
            print(f"‚ö†Ô∏è  Row {row_index}: Could not find 'correctness' in response")
            print(f"üìÑ Response: {response[:200]}...")
            correctness = "incorrect"  # Default fallback
            
        if explanation is None:
            print(f"‚ö†Ô∏è  Row {row_index}: Could not find 'explanation' in response")
            print(f"üìÑ Response: {response[:200]}...")
            explanation = "Failed to parse explanation"  # Default fallback

        return {
            "correctness": correctness,
            "explanation": explanation
        }
        
    except Exception as e:
        print(f"‚ùå Row {row_index}: Parser error: {e}")
        print(f"üìÑ Response: {response[:200]}...")
        return {
            "correctness": "incorrect",
            "explanation": f"Parser error: {e}"
        }


def evaluate_output(dataset, eval_template):
    """Evaluator that checks JSON web page correctness using llm_generate"""

    # Check if template file exists
    template_file = f"{eval_template}.txt"
    if not os.path.exists(template_file):
        raise FileNotFoundError(f"Evaluator template not found: {template_file}")

    # Create the evaluation template
    with open(template_file, "r") as file:
        evaluation_template = file.read()

    # Create the model
    eval_model = OpenAIModel(
        model="gpt-4o",
        model_kwargs={
            "response_format": {"type": "json_object"},
            "temperature": 0
        }
    )

    try:
        # Generate evaluations using llm_generate
        evaluation_results = llm_generate(
            dataframe=dataset,
            template=evaluation_template,
            model=eval_model,
            output_parser=evaluate_output_parser,
            concurrency=20,
            verbose=True
        )
        
        # Debug: Check what columns we actually got
        print(f"üîç Evaluation results columns: {list(evaluation_results.columns)}")
        
        # Check if required columns exist
        missing_cols = []
        for col in ["correctness", "explanation"]:
            if col not in evaluation_results.columns:
                missing_cols.append(col)
        
        if missing_cols:
            print(f"‚ö†Ô∏è  Missing columns in evaluation results: {missing_cols}")
            print(f"üìã Available columns: {list(evaluation_results.columns)}")
            print(f"üìÑ Sample evaluation output:")
            if 'output' in evaluation_results.columns:
                print(evaluation_results['output'].iloc[0] if len(evaluation_results) > 0 else "No results")
            
            # Create dummy columns to prevent crash
            for col in missing_cols:
                evaluation_results[col] = None
                print(f"üîß Created dummy column: {col}")

    except Exception as e:
        print(f"‚ùå Error in llm_generate: {e}")
        print(f"üìÑ Template file: {template_file}")
        print(f"üìä Dataset shape: {dataset.shape}")
        raise

    # Merge the results back into the original dataset
    dataset = dataset.copy()
    for col in ["correctness", "explanation"]:
        if col in evaluation_results.columns:
            dataset[col] = evaluation_results[col]

    return dataset, ["correctness", "explanation"]


def generate_output(dataset, system_prompt):
    output_model = OpenAIModel(
        model="gpt-4.1-2025-04-14",
        model_kwargs={
            "response_format": {"type": "json_object"},
            "temperature": 0
        }
    )
    outputs = llm_generate(
        dataframe=dataset,
        template=system_prompt,
        model=output_model,
        concurrency=20,
        verbose=True
    )
    return outputs["output"]

#test_set.head()

"""## Optimization

There are 3 steps to every loop of optimization.

1. Generate outputs with current prompt on test dataset. Evaluate outputs.
2. If outputs are not satisfactory, generate outputs on training set. Evaluate training outputs.
3. Use training outputs and evaluations to generate optimized prompt.

This process repeats until we see good results on the test set. In this case, we measure our results to be satisfactory when all outputs are deemed "correct" by the evaluate_output evaluator we defined above.
"""

def compute_metric(y_true, y_pred, scorer="accuracy"):
    """
    Compute the requested metric for binary classification.
    y_true and y_pred should be lists or arrays of "correct"/"incorrect" labels.
    scorer: one of "accuracy", "f1", "precision", "recall"
    """
    # Map to binary
    y_true_bin = [1 if y == "correct" else 0 for y in y_true]
    y_pred_bin = [1 if y == "correct" else 0 for y in y_pred]
    if scorer == "accuracy":
        return accuracy_score(y_true_bin, y_pred_bin)
    elif scorer == "f1":
        return f1_score(y_true_bin, y_pred_bin, zero_division=0)
    elif scorer == "precision":
        return precision_score(y_true_bin, y_pred_bin, zero_division=0)
    elif scorer == "recall":
        return recall_score(y_true_bin, y_pred_bin, zero_division=0)
    else:
        raise ValueError(f"Unknown scorer: {scorer}")

def compare_with_targets(outputs, targets, task_type="general"):
    """
    Compare model outputs with ground truth targets.
    
    Args:
        outputs (pd.Series or list): Model outputs (JSON strings)
        targets (pd.Series or list): Ground truth targets
        task_type (str): Type of task for comparison logic
                        - "general": case-insensitive comparison
                        - "sorting": exact case-sensitive comparison (order matters)
                        - "boolean": case-insensitive for True/False
                        - "counting": numeric comparison
        
    Returns:
        float: Accuracy score (0.0 to 1.0)
    """
    correct_count = 0
    total_count = len(outputs)
    
    for output, target in zip(outputs, targets):
        try:
            # Parse JSON output to extract the result
            if isinstance(output, str):
                output_data = json.loads(output)
                predicted_value = output_data.get('result', '')
            else:
                predicted_value = str(output)
            
            # Convert to string for comparison
            predicted_value = str(predicted_value).strip()
            target_value = str(target).strip()
            
            # Clean target value of common formatting (parentheses, brackets, quotes)
            target_value = target_value.strip('()[]"\'')
            
            # Task-specific comparison logic
            if task_type == "sorting":
                # For sorting tasks, exact match required (case and order matter)
                if predicted_value == target_value:
                    correct_count += 1
            elif task_type == "counting":
                # For counting tasks, compare as numbers
                try:
                    pred_num = float(predicted_value)
                    target_num = float(target_value)
                    if pred_num == target_num:
                        correct_count += 1
                except ValueError:
                    # If can't convert to numbers, fall back to string comparison
                    if predicted_value.lower() == target_value.lower():
                        correct_count += 1
            else:
                # General case: case-insensitive comparison (for boolean, yes/no, etc.)
                if predicted_value.lower() == target_value.lower():
                    correct_count += 1
                
        except (json.JSONDecodeError, KeyError, AttributeError) as e:
            # If we can't parse the output, count as incorrect
            continue
    
    return correct_count / total_count if total_count > 0 else 0.0

def get_ground_truth_accuracy(outputs, targets, task_type="general"):
    """
    Direct comparison function for outputs vs targets.
    
    Args:
        outputs: Model outputs (can be from results['raw'][i]['output'])
        targets: Ground truth targets
        task_type: Task type for appropriate comparison logic
        
    Returns:
        float: Accuracy score
    """
    return compare_with_targets(outputs, targets, task_type)

def compare_results_with_targets(results, test_targets, train_targets=None, task_type="general"):
    """
    Convenience function to compare all results with ground truth targets.
    
    Args:
        results: Results dictionary from optimize_loop or simple_test
        test_targets: Ground truth test targets
        train_targets: Ground truth train targets (optional)
        task_type: Task type for appropriate comparison logic
        
    Returns:
        dict: Accuracy scores for each iteration and summary
    """
    comparison = {
        'test_accuracies': [],
        'iteration_details': [],
        'task_type': task_type
    }
    
    # Compare test outputs for each iteration
    if 'raw' in results:
        for i, test_df in enumerate(results['raw']):
            if 'output' in test_df.columns:
                accuracy = get_ground_truth_accuracy(test_df['output'], test_targets, task_type)
                comparison['test_accuracies'].append(accuracy)
                
                # Get LLM evaluator score for this iteration if available
                llm_score = results['test'][i] if i < len(results['test']) else None
                
                comparison['iteration_details'].append({
                    'iteration': i,
                    'ground_truth_accuracy': accuracy,
                    'llm_evaluator_score': llm_score,
                    'difference': abs(accuracy - llm_score) if llm_score is not None else None
                })
    
    # Summary stats
    if comparison['test_accuracies']:
        comparison['initial_accuracy'] = comparison['test_accuracies'][0]
        comparison['final_accuracy'] = comparison['test_accuracies'][-1]
        comparison['improvement'] = comparison['final_accuracy'] - comparison['initial_accuracy']
        comparison['best_accuracy'] = max(comparison['test_accuracies'])
    
    return comparison

def analyze_evaluation_comparison(results_df, ground_truth_comparisons):
    """
    Analyze the difference between LLM evaluator scores and ground truth accuracy.
    
    Args:
        results_df (pd.DataFrame): Results dataframe from experiments  
        ground_truth_comparisons (list): List of comparison dicts from compare_results_with_targets()
        
    Returns:
        pd.DataFrame: Comparison analysis
    """
    analysis_data = []
    
    for idx, row in results_df.iterrows():
        if idx < len(ground_truth_comparisons):
            comparison = ground_truth_comparisons[idx]
            
            final_llm_score = row['test'][-1] if isinstance(row['test'], list) else row['test']
            initial_llm_score = row['test'][0] if isinstance(row['test'], list) else row['test']
            
            final_ground_truth_acc = comparison['final_accuracy']
            initial_ground_truth_acc = comparison['initial_accuracy']
            
            file_name = row.get('file', 'unknown')
            task_name = comparison.get('task', f'task_{idx}')
            
            analysis_data.append({
                'experiment': idx,
                'task': task_name,
                'evaluator_template': file_name,
                'initial_llm_score': initial_llm_score,
                'initial_ground_truth_accuracy': initial_ground_truth_acc,
                'final_llm_score': final_llm_score,
                'final_ground_truth_accuracy': final_ground_truth_acc,
                'llm_improvement': final_llm_score - initial_llm_score,
                'ground_truth_improvement': final_ground_truth_acc - initial_ground_truth_acc,
                'final_difference': abs(final_llm_score - final_ground_truth_acc),
                'agreement': 'High' if abs(final_llm_score - final_ground_truth_acc) < 0.1 else 'Low'
            })
    
    return pd.DataFrame(analysis_data)

def optimize_loop(
    train_set,
    test_set,
    system_prompt,
    eval_template,
    evaluators,
    threshold=1,
    loops=NUM_OPTIMIZATION_LOOPS,
    scorer="accuracy"
):
    """
    Run prompt optimization loop with LLM evaluator.
    
    Args:
        scorer: one of "accuracy", "f1", "precision", "recall"
        threshold: float, threshold for the selected metric
        loops: number of optimization iterations to run

    Returns:
        dict with keys:
            "train": list of train set scores per run
            "test": list of test set scores per run
            "prompt": list of system prompts used for each test run
            "raw": list of test set DataFrames (deepcopy) for each test run
            "file": evaluator template used
    """
    import copy

    curr_loop = 1
    train_metrics = []
    test_metrics = []
    prompts = []
    raw_dfs = []

    print(f"üöÄ Starting prompt optimization with {loops} iterations (scorer: {scorer}, threshold: {threshold})")
    print()
    
    # Initial test evaluation before optimization
    print(f"üìä Initial evaluation:")
    test_set["output"] = generate_output(test_set, system_prompt)
    test_evals_all = evaluate_output(test_set, eval_template)[0]
    test_evals = test_evals_all["correctness"]
    y_true = ["correct"] * len(test_evals)
    y_pred = test_evals
    initial_metric_value = compute_metric(y_true, y_pred, scorer=scorer)
    test_metrics.append(initial_metric_value)
    prompts.append(system_prompt)
    raw_dfs.append(copy.deepcopy(test_set))
    
    print(f"‚úÖ Initial test {scorer}: {initial_metric_value}")
    print('\n')
    
    if initial_metric_value >= threshold:
        print(f"üéâ Initial prompt already meets threshold!")
        result = {
            "initial metric": initial_metric_value,
            "train": train_metrics,
            "test": test_metrics,
            "prompt": prompts,
            "file": eval_template,
            "raw": raw_dfs
        }
        return result
    
    while loops > 0:
        print(f"üìä Loop {curr_loop}: Optimizing prompt...")
        
        # 1. Train set evaluation and optimization
        train_outputs = generate_output(train_set, system_prompt)
        train_set["output"] = train_outputs

        train_set["correctness"] = [None] * len(train_set)
        train_set["explanation"] = [None] * len(train_set)
        train_set["rule_violations"] = [None] * len(train_set)

        optimizer = PromptLearningOptimizer(
            prompt=system_prompt,
            model_choice="gpt-4o",
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )

        # Prepare evaluators for the optimization run
        evaluators_with_rules = []
        for evaluator in evaluators:
            if evaluator.__name__ == 'evaluate_output':
                evaluators_with_rules.append(lambda ds: evaluate_output(ds, eval_template))
            else:
                evaluators_with_rules.append(evaluator)
        
        train_set, _ = optimizer.run_evaluators(
            train_set,
            evaluators_with_rules,
            feedback_columns=["correctness", "explanation", "rule_violations"]
        )

        system_prompt = optimizer.optimize(
            train_set,
            "output",
            feedback_columns=["correctness", "explanation", "rule_violations"],
            context_size_k=128000
        )

        # Evaluate train set after optimization
        train_outputs_post = generate_output(train_set, system_prompt)
        train_set_post = train_set.copy()
        train_set_post["output"] = train_outputs_post
        train_evals_post_all = evaluate_output(train_set_post, eval_template)[0]
        train_evals_post = train_evals_post_all["correctness"]
        y_true_train_post = ["correct"] * len(train_evals_post)
        y_pred_train_post = train_evals_post
        train_metric_post_value = compute_metric(y_true_train_post, y_pred_train_post, scorer=scorer)
        train_metrics.append(train_metric_post_value)
        print(f"‚úÖ Train {scorer}: {train_metric_post_value}")

        # 2. Test set evaluation with optimized prompt
        test_set["output"] = generate_output(test_set, system_prompt)
        
        test_evals_all = evaluate_output(test_set, eval_template)[0]
        test_evals = test_evals_all["correctness"]
        y_true = ["correct"] * len(test_evals)
        y_pred = test_evals
        metric_value = compute_metric(y_true, y_pred, scorer=scorer)
        test_metrics.append(metric_value)
        prompts.append(system_prompt)
        raw_dfs.append(copy.deepcopy(test_set))
        
        print(f"‚úÖ Test {scorer}: {metric_value}")
        print("\n")
        
        # 3. Check threshold
        if metric_value >= threshold:
            print(f"üéâ Threshold reached! Stopping optimization.")
            result = {
                "initial metric": initial_metric_value,
                "train": train_metrics,
                "test": test_metrics,
                "prompt": prompts,
                "file": eval_template,
                "raw": raw_dfs
            }
            return result

        loops -= 1
        curr_loop += 1

    print(f"üîÑ All {curr_loop-1} optimization loops completed.")
    result = {
        "initial metric": initial_metric_value,
        "train": train_metrics,
        "test": test_metrics,
        "prompt": prompts,
        "file": eval_template,
        "raw": raw_dfs
    }
    return result



def save_experiment_results(results, filename="experiment_results.json"):
    """Save experiment results to a JSON file."""
    from datetime import datetime
    
    # Add timestamp to results
    results_with_timestamp = {
        "timestamp": datetime.now().isoformat(),
        "results": results
    }
    
    with open(filename, 'w') as f:
        json.dump(results_with_timestamp, f, indent=2, default=str)
    
    print(f"‚úÖ Results saved to {filename}")



def simple_test(train_set, test_set, system_prompt, eval_template, results_df, threshold=1, loops=NUM_OPTIMIZATION_LOOPS, scorer="accuracy"):
    """
    Run prompt optimization experiment with LLM evaluator only.
    For ground truth comparison, use compare_results_with_targets() afterwards.
    
    Args:
        loops: Number of optimization loops (defaults to NUM_OPTIMIZATION_LOOPS config)
    """
    evaluators = [evaluate_output]
    results = optimize_loop(
        train_set, test_set, system_prompt, eval_template, evaluators,
        threshold=threshold,
        loops=loops,
        scorer=scorer
    )
    print("‚úÖ Completed experiment")
    print(f"   Initial metric: {results['initial metric']:.3f}")
    print(f"   Final test {scorer}: {results['test'][-1]:.3f}")
    print(f"   Final prompt: {results['prompt'][-1]}")
    
    results_df = pd.concat([results_df, pd.DataFrame({k: [v] for k, v in results.items()})], ignore_index=True)
    return results, results_df

wol_prompt = "You are an expert in solving truthfulness puzzles. Return your answer **in JSON** with a single key `result` whose value is either \"Yes\" or \"No\". This is your task: {input}"
bool_prompt = "You are an expert in solving boolean expressions. Return your answer **in JSON** with a single key `result` whose value is either \"True\" or \"False\". This is your task: {input}"
word_sorting_prompt = "You are an expert in sorting words alphabetically. Return your answer **in JSON** with a single key `result` whose value is the alphabetically sorted list of words separated by spaces. This is your task: {input}"
sports_prompt = "You are an expert in understanding sports. Return your answer **in JSON** with a single key `result` whose value is either \"Yes\" or \"No\". This is your task: {input}"
object_prompt = "You are an expert in counting objects. Return your answer **in JSON** with a single key `result` whose value is the number of objects in the input. This is your task: {input}"

# Comprehensive prompts for all BigBench Hard tasks
causal_prompt = "You are an expert in causal reasoning. Analyze the given scenario and determine the causal relationship. Return your answer **in JSON** with a single key `result`. This is your task: {input}"
date_prompt = "You are an expert in date understanding and temporal reasoning. Return your answer **in JSON** with a single key `result`. This is your task: {input}"
disambiguation_prompt = "You are an expert in disambiguation and question answering. Return your answer **in JSON** with a single key `result`. This is your task: {input}"
dyck_prompt = "You are an expert in formal language theory and parsing Dyck languages. Return your answer **in JSON** with a single key `result`. This is your task: {input}"
fallacies_prompt = "You are an expert in identifying formal fallacies in logical arguments. Return your answer **in JSON** with a single key `result` whose value is either \"valid\" or \"invalid\". This is your task: {input}"
geometric_prompt = "You are an expert in geometric reasoning and shape analysis. Return your answer **in JSON** with a single key `result`. This is your task: {input}"
hyperbaton_prompt = "You are an expert in syntax and understanding hyperbaton (word order variations). Return your answer **in JSON** with a single key `result`. This is your task: {input}"
logical_deduction_prompt = "You are an expert in logical deduction and reasoning. Return your answer **in JSON** with a single key `result`. This is your task: {input}"
movie_prompt = "You are an expert in movie recommendations and entertainment preferences. Return your answer **in JSON** with a single key `result`. This is your task: {input}"
arithmetic_prompt = "You are an expert in multi-step arithmetic calculations. Return your answer **in JSON** with a single key `result` whose value is the final numerical answer. This is your task: {input}"
navigate_prompt = "You are an expert in navigation and spatial reasoning. Return your answer **in JSON** with a single key `result`. This is your task: {input}"
penguins_prompt = "You are an expert in table reasoning and data analysis. Return your answer **in JSON** with a single key `result`. This is your task: {input}"
colored_objects_prompt = "You are an expert in reasoning about colored objects and their properties. Return your answer **in JSON** with a single key `result`. This is your task: {input}"
ruin_names_prompt = "You are an expert in name corruption and string manipulation. Return your answer **in JSON** with a single key `result`. This is your task: {input}"
translation_prompt = "You are an expert in detecting translation errors. Return your answer **in JSON** with a single key `result` whose value is either \"Yes\" or \"No\". This is your task: {input}"
snarks_prompt = "You are an expert in solving logical puzzles and snarks. Return your answer **in JSON** with a single key `result`. This is your task: {input}"
temporal_prompt = "You are an expert in temporal sequence reasoning and time-based logic. Return your answer **in JSON** with a single key `result`. This is your task: {input}"
tracking_prompt = "You are an expert in tracking shuffled objects and spatial reasoning. Return your answer **in JSON** with a single key `result`. This is your task: {input}"

# Use run_bbh_experiments() to run experiments on BigBench Hard tasks.


def check_evaluator_templates(task_mappings):
    """Check if all required evaluator template files exist."""
    missing_templates = []
    
    # Get unique evaluator templates from task mappings
    unique_templates = set(mapping[0] for mapping in task_mappings.values())
    
    for template in unique_templates:
        template_file = f"{template}.txt"
        if not os.path.exists(template_file):
            missing_templates.append(template_file)
    
    if missing_templates:
        print(f"‚ùå Missing evaluator template files:")
        for template in missing_templates:
            print(f"   ‚Ä¢ {template}")
        print(f"\nüîß Please create these template files before running experiments.")
        return False
    
    print(f"‚úÖ All {len(unique_templates)} evaluator template files found")
    return True

def run_bbh_experiments():
    """
    Function to run experiments on BigBench Hard tasks.
    Call this function to download JSON files and run experiments.
    """
    # Download files if not already downloaded
    if not os.path.exists("bbh-download") or len(get_available_bbh_tasks("bbh-download")) == 0:
        print("Downloading BigBench Hard JSON files...")
        downloaded_files = download_bbh_json_files("bbh-download")
    '''
    # Define task mappings (evaluator, prompt, task_type)
    task_mappings = {
        "boolean_expressions": ("evaluator-bool", bool_prompt, "boolean"),
        "web_of_lies": ("evaluator-lies", wol_prompt, "general"),
        "word_sorting": ("evaluator-wordsort", word_sorting_prompt, "sorting"),
        "sports_understanding": ("evaluator-sports", sports_prompt, "general"),
        "object_counting": ("evaluator-object", object_prompt, "counting"),
        # All 27 BigBench Hard tasks
        "causal_judgement": ("evaluator-causal", causal_prompt, "general"),
        "date_understanding": ("evaluator-date", date_prompt, "general"),
        "disambiguation_qa": ("evaluator-disambiguation", disambiguation_prompt, "general"),
        "dyck_languages": ("evaluator-dyck", dyck_prompt, "general"),
        "formal_fallacies": ("evaluator-fallacies", fallacies_prompt, "general"),
        "geometric_shapes": ("evaluator-geometric", geometric_prompt, "general"),
        "hyperbaton": ("evaluator-hyperbaton", hyperbaton_prompt, "general"),
        "logical_deduction_five_objects": ("evaluator-logical", logical_deduction_prompt, "general"),
        "logical_deduction_seven_objects": ("evaluator-logical", logical_deduction_prompt, "general"),
        "logical_deduction_three_objects": ("evaluator-logical", logical_deduction_prompt, "general"),
        "movie_recommendation": ("evaluator-movie", movie_prompt, "general"),
        "multistep_arithmetic_two": ("evaluator-arithmetic", arithmetic_prompt, "counting"),
        "navigate": ("evaluator-navigate", navigate_prompt, "general"),
        "penguins_in_a_table": ("evaluator-penguins", penguins_prompt, "general"),
        "reasoning_about_colored_objects": ("evaluator-colored", colored_objects_prompt, "general"),
        "ruin_names": ("evaluator-ruin", ruin_names_prompt, "general"),
        "salient_translation_error_detection": ("evaluator-translation", translation_prompt, "general"),
        "snarks": ("evaluator-snarks", snarks_prompt, "general"),
        "temporal_sequences": ("evaluator-temporal", temporal_prompt, "general"),
        "tracking_shuffled_objects_five_objects": ("evaluator-tracking", tracking_prompt, "general"),
        "tracking_shuffled_objects_seven_objects": ("evaluator-tracking", tracking_prompt, "general"),
        "tracking_shuffled_objects_three_objects": ("evaluator-tracking", tracking_prompt, "general"),
    }

    task_mappings = {
        "date_understanding": ("evaluator-date", date_prompt, "general"),
        "disambiguation_qa": ("evaluator-disambiguation", disambiguation_prompt, "general"),
        "dyck_languages": ("evaluator-dyck", dyck_prompt, "general"),
        "geometric_shapes": ("evaluator-geometric", geometric_prompt, "general"),
        "hyperbaton": ("evaluator-hyperbaton", hyperbaton_prompt, "general"),
        "logical_deduction_five_objects": ("evaluator-logical", logical_deduction_prompt, "general"),
        "logical_deduction_seven_objects": ("evaluator-logical", logical_deduction_prompt, "general"),
        "logical_deduction_three_objects": ("evaluator-logical", logical_deduction_prompt, "general"),
        "movie_recommendation": ("evaluator-movie", movie_prompt, "general"),
        "penguins_in_a_table": ("evaluator-penguins", penguins_prompt, "general"),
        "reasoning_about_colored_objects": ("evaluator-colored", colored_objects_prompt, "general"),
        "ruin_names": ("evaluator-ruin", ruin_names_prompt, "general"),
        "salient_translation_error_detection": ("evaluator-translation", translation_prompt, "general"),
        "snarks": ("evaluator-snarks", snarks_prompt, "general"),
        "temporal_sequences": ("evaluator-temporal", temporal_prompt, "general"),
        "tracking_shuffled_objects_five_objects": ("evaluator-tracking", tracking_prompt, "general"),
        "tracking_shuffled_objects_seven_objects": ("evaluator-tracking", tracking_prompt, "general"),
        "tracking_shuffled_objects_three_objects": ("evaluator-tracking", tracking_prompt, "general"),
    }
        '''

    task_mappings = {
        "boolean_expressions": ("evaluator-bool", bool_prompt, "boolean"),
        "web_of_lies": ("evaluator-lies", wol_prompt, "general"),
        "word_sorting": ("evaluator-wordsort", word_sorting_prompt, "sorting"),
        "sports_understanding": ("evaluator-sports", sports_prompt, "general"),
        "object_counting": ("evaluator-object", object_prompt, "counting"),
        # All 27 BigBench Hard tasks
        "causal_judgement": ("evaluator-causal", causal_prompt, "general"),
        "date_understanding": ("evaluator-date", date_prompt, "general"),
        "disambiguation_qa": ("evaluator-disambiguation", disambiguation_prompt, "general"),
        #"dyck_languages": ("evaluator-dyck", dyck_prompt, "general"),
        "formal_fallacies": ("evaluator-fallacies", fallacies_prompt, "general"),
        "geometric_shapes": ("evaluator-geometric", geometric_prompt, "general"),
        "hyperbaton": ("evaluator-hyperbaton", hyperbaton_prompt, "general"),
        "logical_deduction_five_objects": ("evaluator-logical", logical_deduction_prompt, "general"),
        "logical_deduction_seven_objects": ("evaluator-logical", logical_deduction_prompt, "general"),
        "logical_deduction_three_objects": ("evaluator-logical", logical_deduction_prompt, "general"),
        #"movie_recommendation": ("evaluator-movie", movie_prompt, "general"),
        "multistep_arithmetic_two": ("evaluator-arithmetic", arithmetic_prompt, "counting"),
        #"navigate": ("evaluator-navigate", navigate_prompt, "general"),
        "penguins_in_a_table": ("evaluator-penguins", penguins_prompt, "general"),
        "reasoning_about_colored_objects": ("evaluator-colored", colored_objects_prompt, "general"),
        #"ruin_names": ("evaluator-ruin", ruin_names_prompt, "general"),
        "salient_translation_error_detection": ("evaluator-translation", translation_prompt, "general"),
        "snarks": ("evaluator-snarks", snarks_prompt, "general"),
        "temporal_sequences": ("evaluator-temporal", temporal_prompt, "general"),
        "tracking_shuffled_objects_five_objects": ("evaluator-tracking", tracking_prompt, "general"),
        "tracking_shuffled_objects_seven_objects": ("evaluator-tracking", tracking_prompt, "general"),
        "tracking_shuffled_objects_three_objects": ("evaluator-tracking", tracking_prompt, "general"),
    }
    # Check if all evaluator templates exist before starting
    if not check_evaluator_templates(task_mappings):
        raise FileNotFoundError("Missing evaluator template files. Cannot run experiments.")
    
    results_df = pd.DataFrame(columns=["initial metric", "train", "test", "prompt", "file", "raw"])
    all_comparisons = []  # Store ground truth comparisons separately
    
    # Run experiments for each task
    for task_name, (eval_template, prompt, task_type) in task_mappings.items():
        json_file_path = f"bbh-download/{task_name}.json"
        if os.path.exists(json_file_path):
            print(f"\nüî¨ Running experiment for {task_name}...")
            dataset, train_set, test_set, train_targets, test_targets = data_prep_json(json_file_path)
            
            # Run the optimization experiment (clean, no targets)
            results, results_df = simple_test(train_set, test_set, prompt, eval_template, results_df)
            
            # Do ground truth comparison separately with appropriate task type
            print(f"üìä Comparing with ground truth (task_type: {task_type})...")
            comparison = compare_results_with_targets(results, test_targets, task_type=task_type)
            comparison['task'] = task_name
            comparison['eval_template'] = eval_template

            # Check if we should skip this task due to all generations failing (final_accuracy == 0)
            if SKIP_ZERO_ACCURACY_TASKS and comparison['final_accuracy'] == 0.0:
                print(f"‚ö†Ô∏è  Skipping {task_name} from results: all generations failed (final_accuracy=0.0)")
                continue  # Do not append to all_comparisons or results_df

            all_comparisons.append(comparison)
            # Print ground truth results
            print(f"   Initial ground truth accuracy: {comparison['initial_accuracy']:.3f}")
            print(f"   Final ground truth accuracy: {comparison['final_accuracy']:.3f}")
            print(f"   Ground truth improvement: {comparison['improvement']:.3f}")
            
        else:
            print(f"‚ö†Ô∏è  Skipping {task_name} - file not found")
    
    # Save results
    results_df.to_csv("bbh_results.csv")
    print("\n‚úÖ LLM evaluator results saved to bbh_results.csv")
    
    # Save ground truth comparison results
    if all_comparisons:
        comparison_df = pd.DataFrame([
            {
                'task': comp['task'],
                'eval_template': comp['eval_template'],
                'initial_gt_accuracy': comp['initial_accuracy'],
                'final_gt_accuracy': comp['final_accuracy'],
                'gt_improvement': comp['improvement'],
                'best_gt_accuracy': comp['best_accuracy']
            }
            for comp in all_comparisons
        ])
        comparison_df.to_csv("bbh_ground_truth_comparison.csv", index=False)
        print("‚úÖ Ground truth comparison saved to bbh_ground_truth_comparison.csv")
        
        # Create summary table with LLM evaluator scores
        summary_data = []
        for i, (comp, (idx, row)) in enumerate(zip(all_comparisons, results_df.iterrows())):
            final_llm_score = row['test'][-1] if isinstance(row['test'], list) and len(row['test']) > 0 else 0.0
            initial_llm_score = row['test'][0] if isinstance(row['test'], list) and len(row['test']) > 0 else 0.0
            
            summary_data.append({
                'Task': comp['task'],
                'Initial_Ground_Truth_Accuracy': comp['initial_accuracy'],
                'Final_Ground_Truth_Accuracy': comp['final_accuracy'],
                'Initial_LLM_Evaluator_Score': initial_llm_score,
                'Final_LLM_Evaluator_Score': final_llm_score,
                'Ground_Truth_Improvement': comp['improvement'],
                'LLM_Evaluator_Improvement': final_llm_score - initial_llm_score,
                'Task_Type': comp['task_type']
            })
        
        summary_df = pd.DataFrame(summary_data)
        
        # Print formatted table
        print(f"\nüìä EXPERIMENT SUMMARY TABLE")
        print("=" * 100)
        print(f"{'Task':<25} {'Final GT':<9} {'Init GT':<8} {'GT Œî':<7} {'Final LLM':<10} {'Init LLM':<9} {'LLM Œî':<8} {'Type':<8}")
        print("-" * 100)
        
        for _, row in summary_df.iterrows():
            print(f"{row['Task']:<25} {row['Final_Ground_Truth_Accuracy']:<9.3f} "
                  f"{row['Initial_Ground_Truth_Accuracy']:<8.3f} {row['Ground_Truth_Improvement']:<7.3f} "
                  f"{row['Final_LLM_Evaluator_Score']:<10.3f} {row['Initial_LLM_Evaluator_Score']:<9.3f} "
                  f"{row['LLM_Evaluator_Improvement']:<8.3f} {row['Task_Type']:<8}")
        
        print("-" * 100)
        print(f"{'AVERAGE':<25} {summary_df['Final_Ground_Truth_Accuracy'].mean():<9.3f} "
              f"{summary_df['Initial_Ground_Truth_Accuracy'].mean():<8.3f} "
              f"{summary_df['Ground_Truth_Improvement'].mean():<7.3f} "
              f"{summary_df['Final_LLM_Evaluator_Score'].mean():<10.3f} "
              f"{summary_df['Initial_LLM_Evaluator_Score'].mean():<9.3f} "
              f"{summary_df['LLM_Evaluator_Improvement'].mean():<8.3f}")
        
        # Save summary table (not in .gitignore)
        summary_df.to_csv("experiment_summary_table.txt", index=False, sep='\t')
        print(f"\n‚úÖ Summary table saved to experiment_summary_table.txt")
        
        # Save detailed raw comparison data (not in .gitignore)
        os.makedirs("raw_comparison_data", exist_ok=True)
        
        for i, comp in enumerate(all_comparisons):
            task_name = comp['task']
            
            # Save iteration details for this task
            details_df = pd.DataFrame(comp['iteration_details'])
            details_filename = f"raw_comparison_data/{task_name}_iteration_details.txt"
            details_df.to_csv(details_filename, index=False, sep='\t')
            
            # Save test accuracies for this task
            accuracies_data = {
                'iteration': list(range(len(comp['test_accuracies']))),
                'ground_truth_accuracy': comp['test_accuracies'],
                'task_type': [comp['task_type']] * len(comp['test_accuracies'])
            }
            accuracies_df = pd.DataFrame(accuracies_data)
            accuracies_filename = f"raw_comparison_data/{task_name}_ground_truth_accuracies.txt"
            accuracies_df.to_csv(accuracies_filename, index=False, sep='\t')
        
        print(f"‚úÖ Raw comparison data saved to raw_comparison_data/ directory")
        print(f"   üìÅ Files: {len(all_comparisons) * 2} detailed files (iteration details + accuracies per task)")
        
        # Print final summary
        print(f"\nüìà Overall Results:")
        print(f"   Average final ground truth accuracy: {summary_df['Final_Ground_Truth_Accuracy'].mean():.3f}")
        print(f"   Average ground truth improvement: {summary_df['Ground_Truth_Improvement'].mean():.3f}")
        print(f"   Average final LLM evaluator score: {summary_df['Final_LLM_Evaluator_Score'].mean():.3f}")
        print(f"   Average LLM evaluator improvement: {summary_df['LLM_Evaluator_Improvement'].mean():.3f}")
        print(f"   Best performing task (GT): {summary_df.loc[summary_df['Final_Ground_Truth_Accuracy'].idxmax(), 'Task']}")
        print(f"   Best performing task (LLM): {summary_df.loc[summary_df['Final_LLM_Evaluator_Score'].idxmax(), 'Task']}")
    
    return results_df, all_comparisons, summary_df

# To run BigBench Hard experiments, uncomment the following line:
# bbh_results = run_bbh_experiments()

# =============================================================================
# GETTING STARTED
# =============================================================================
print("üöÄ BigBench Hard JSON Integration Loaded!")
print("üìñ For usage instructions, see README_BBH.md")
print("")
print("‚öôÔ∏è  Configuration: Edit NUM_OPTIMIZATION_LOOPS at top of file to control experiment iterations")
print("")
print("üìã ALL 27 BIGBENCH HARD TASKS CONFIGURED:")
print("   ‚Ä¢ Boolean expressions, Web of lies, Word sorting, Sports understanding, Object counting")
print("   ‚Ä¢ Causal judgement, Date understanding, Disambiguation QA, Dyck languages")
print("   ‚Ä¢ Formal fallacies, Geometric shapes, Hyperbaton, Logical deduction (3/5/7 objects)")
print("   ‚Ä¢ Movie recommendation, Multistep arithmetic, Navigate, Penguins in table") 
print("   ‚Ä¢ Reasoning about colored objects, Ruin names, Translation error detection")
print("   ‚Ä¢ Snarks, Temporal sequences, Tracking shuffled objects (3/5/7 objects)")
print("   ‚úÖ All prompts and evaluator templates ready!")
print("")
print("Quick start:")
print("1. Test download: python test_bbh_download.py")
print("2. Run ALL experiments: python pl_multidataset.py")
print("3. Or import: from pl_multidataset import run_bbh_experiments; run_bbh_experiments()")
print("")
print("Available functions:")
print("- download_bbh_json_files() - Download BigBench Hard JSON files")
print("- get_available_bbh_tasks() - List available tasks") 
print("- data_prep_json() - Prepare data from JSON files (targets removed & saved separately)")
print("- run_bbh_experiments() - Run complete experiments on ALL 27 tasks")
print("- compare_with_targets() - Compare model outputs with ground truth")
print("- get_ground_truth_accuracy() - Direct comparison: outputs vs targets")
print("- compare_results_with_targets() - Convenience function for full results comparison")
print("- analyze_evaluation_comparison() - Compare LLM evaluator vs ground truth scores")
print("")
print("üÜï NEW: Complete BigBench Hard Integration")
print("     ‚úÖ All 27 tasks with specialized prompts and evaluators")
print("     ‚úÖ Task-specific accuracy comparison (boolean, sorting, counting, general)")
print("     ‚úÖ Comprehensive summary tables and raw data export")
print("     ‚úÖ Clean separation: LLM evaluation vs ground truth comparison")
print("     ‚úÖ Enhanced reporting with detailed metrics per task")
print("")
print("üöÄ Ready to run ALL BigBench Hard experiments!")
print("=" * 80)

# =============================================================================
# MAIN EXECUTION - Run all BigBench Hard experiments
# =============================================================================

def main():
    """Main function to run all BigBench Hard experiments."""
    print("\n" + "=" * 80)
    print("üöÄ STARTING COMPREHENSIVE BIGBENCH HARD EXPERIMENTS")
    print("=" * 80)
    print(f"‚öôÔ∏è  Configuration: {NUM_OPTIMIZATION_LOOPS} optimization loops per experiment")
    print(f"üìä Sampling: {NUM_SAMPLES if NUM_SAMPLES > 0 else 'ALL'} samples per task")
    print(f"üîÄ Train/Test split: {TRAIN_SPLIT_FRACTION:.1%} train, {1-TRAIN_SPLIT_FRACTION:.1%} test")
    print("")
    print("üìã RUNNING ALL 27 BIGBENCH HARD TASKS:")
    print("   Logic & Reasoning:")
    print("   ‚Ä¢ boolean_expressions, formal_fallacies, logical_deduction_*")
    print("   ‚Ä¢ dyck_languages, snarks, hyperbaton")
    print("")
    print("   Temporal & Spatial:")
    print("   ‚Ä¢ date_understanding, temporal_sequences, navigate")
    print("   ‚Ä¢ tracking_shuffled_objects_*, geometric_shapes")
    print("")
    print("   Language & Knowledge:")
    print("   ‚Ä¢ web_of_lies, word_sorting, sports_understanding")
    print("   ‚Ä¢ disambiguation_qa, movie_recommendation, ruin_names")
    print("   ‚Ä¢ salient_translation_error_detection")
    print("")
    print("   Math & Counting:")
    print("   ‚Ä¢ object_counting, multistep_arithmetic_two")
    print("")
    print("   Complex Reasoning:")
    print("   ‚Ä¢ causal_judgement, reasoning_about_colored_objects")
    print("   ‚Ä¢ penguins_in_a_table")
    print("")
    print("üéØ Total Experiments: 27 BigBench Hard tasks")
    print("‚è±Ô∏è  Estimated time: 15-30 minutes (depending on configuration)")
    print("=" * 80)
    
    try:
        results_df, all_comparisons, summary_df = run_bbh_experiments()
        
        print("\n" + "=" * 80)
        print("üéâ ALL 27 BIGBENCH HARD EXPERIMENTS COMPLETED SUCCESSFULLY!")
        print("=" * 80)
        print("üìÅ Results saved:")
        print("   ‚Ä¢ bbh_results.csv (LLM evaluator results)")
        print("   ‚Ä¢ bbh_ground_truth_comparison.csv (ground truth comparison)")
        print("   ‚Ä¢ experiment_summary_table.txt (summary table)")
        print("   ‚Ä¢ raw_comparison_data/ (detailed iteration data for all 27 tasks)")
        print("")
        print(f"üìä Summary:")
        print(f"   ‚Ä¢ Tasks completed: {len(results_df)}")
        print(f"   ‚Ä¢ Average final accuracy: {summary_df['Final_Ground_Truth_Accuracy'].mean():.3f}")
        print(f"   ‚Ä¢ Average improvement: {summary_df['Ground_Truth_Improvement'].mean():.3f}")
        print(f"   ‚Ä¢ Best performing task: {summary_df.loc[summary_df['Final_Ground_Truth_Accuracy'].idxmax(), 'Task']}")
        print("=" * 80)
        
        return results_df, all_comparisons, summary_df
        
    except Exception as e:
        print(f"\n‚ùå ERROR during experiments: {e}")
        print("üîß Check that:")
        print("   ‚Ä¢ OpenAI API key is set: export OPENAI_API_KEY=your_key")
        print("   ‚Ä¢ All evaluator templates exist (should be 24 evaluator-*.txt files)")
        print("   ‚Ä¢ BigBench Hard JSON files are available (will auto-download)")
        print("   ‚Ä¢ Virtual environment is activated and dependencies installed")
        raise

if __name__ == "__main__":
    # Run all experiments when script is executed directly
    main()