# -*- coding: utf-8 -*-
"""prompt_learning_cookbook_AX.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IhN8oZJOxkft9eLY8pJte5nWdFlbl0C4

[![My Image](data:image/png;base64,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)](https://arize.com)

# Optimizing JSON Webpage Prompts with the Arize Prompt Learning SDK

In this cookbook, we demonstrate a use case of the Arize Prompt Learning SDK by optimizing a system prompt for GPT-4.1. The goal is to improve the model’s ability to generate accurate JSON representations of webpages in response to user queries. The dataset consists of prompts asking GPT to generate webpages, and we define 10 specific rules that the JSON outputs must satisfy. Using the SDK, we iteratively refine the prompt to achieve high accuracy on the training set, and then evaluate its performance on a separate test set.
"""
import pandas as pd
from phoenix.evals import llm_generate
from phoenix.evals import OpenAIModel
import os
import getpass
import re
import json
import nest_asyncio
import openai
from arize_toolkit.extensions.prompt_optimizer import PromptLearningOptimizer
from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score
import requests
import urllib.parse
from pathlib import Path
#!pip install arize-phoenix-evals arize-phoenix-client tiktoken openai arize-toolkit

# CONFIG: Experiment settings - adjust as needed
NUM_SAMPLES = 100  # Number of rows to sample from the full dataset, 0 for all
TRAIN_SPLIT_FRACTION = 0.5  # Fraction of data to use for training (rest for testing)
NUM_RULES = 50  # Number of rules in the prompt - adjust based on your evaluator prompt (this is NOT working on Config)

# EXPERIMENT CONFIGURATION
RUN_MULTI_RULE_EXPERIMENTS = False  # Set to True to run experiments with multiple rule counts
RULE_COUNTS_TO_TEST = [10, 50, 100]  # Rule counts to test in multi-rule experiments
NUM_OPTIMIZATION_LOOPS = 1  # Number of optimization loops per experiment (used by simple_test and optimize_loop)

# USAGE EXAMPLES:
# 1. Single experiment with 50 rules (default):
#    - Set RUN_MULTI_RULE_EXPERIMENTS = False
#    - Results saved to "single_experiment_results.json"
#
# 2. Multi-rule experiments:
#    - Set RUN_MULTI_RULE_EXPERIMENTS = True
#    - Adjust RULE_COUNTS_TO_TEST as needed
#    - Results saved to "multi_rule_experiments.json"
#
# 3. Load previous results:
#    - Use load_experiment_results("filename.json")
#
# 4. Control optimization loops:
#    - Set NUM_OPTIMIZATION_LOOPS to control how many iterations per experiment (affects both simple_test and optimize_loop)

import nest_asyncio, re
nest_asyncio.apply()

# 1️⃣  stricter variable detector
from phoenix.evals.templates import PromptTemplate, PromptPartTemplate
_TEMPLATE_RE = re.compile(r"\{([a-zA-Z_][a-zA-Z0-9_]*)\}")
def _parse_variables_strict(self, tmpl: list[PromptPartTemplate]):  # [...]
    vars = set()
    for p in tmpl:
        vars.update(m.group(1) for m in _TEMPLATE_RE.finditer(p.template))
    return list(vars)
PromptTemplate._parse_variables = _parse_variables_strict

# 2️⃣  literal‑brace formatter
from phoenix.evals.templates import PromptPart, MultimodalPrompt
def _format_literal(self, variable_values, options=None):  # [...]
    prompt_msgs = []
    for part in self.prompt(options):
        msg = part.template
        for var in self.variables:
            msg = msg.replace(f"{{{var}}}", str(variable_values[var]))
        prompt_msgs.append(PromptPart(content_type=part.content_type, content=msg))
    return MultimodalPrompt(parts=prompt_msgs)
PromptTemplate.format = _format_literal

"""## OpenAI Key
We will be using OpenAI to generate the webpage jsons.
"""

client = openai.Client(api_key=os.getenv("OPENAI_API_KEY"))

"""## Training and Test Datasets

Create training and test datasets, and export to Arize.

First, download the [dataset of queries](https://storage.googleapis.com/arize-assets/dev-rel/prompt-learning/queries.csv).
"""
def download_bbh_json_files(download_dir="bbh-download"):
    """
    Download all JSON files from BigBench Hard repository.
    
    Args:
        download_dir (str): Directory to download files to
        
    Returns:
        list: List of downloaded file paths
    """
    os.makedirs(download_dir, exist_ok=True)
    
    # Known BigBench Hard tasks from the literature
    bbh_tasks = [
        "boolean_expressions",
        "causal_judgement", 
        "date_understanding",
        "disambiguation_qa",
        "dyck_languages",
        "formal_fallacies",
        "geometric_shapes",
        "hyperbaton",
        "logical_deduction_five_objects",
        "logical_deduction_seven_objects",
        "logical_deduction_three_objects",
        "movie_recommendation",
        "multistep_arithmetic_two",
        "navigate",
        "object_counting",
        "penguins_in_a_table",
        "reasoning_about_colored_objects",
        "ruin_names",
        "salient_translation_error_detection",
        "snarks",
        "sports_understanding",
        "temporal_sequences",
        "tracking_shuffled_objects_five_objects",
        "tracking_shuffled_objects_seven_objects",
        "tracking_shuffled_objects_three_objects",
        "web_of_lies",
        "word_sorting"
    ]
    
    base_url = "https://raw.githubusercontent.com/suzgunmirac/BIG-Bench-Hard/main/bbh"
    downloaded_files = []
    
    for task_name in bbh_tasks:
        file_url = f"{base_url}/{task_name}.json"
        local_path = os.path.join(download_dir, f"{task_name}.json")
        
        try:
            print(f"Downloading {task_name}.json...")
            response = requests.get(file_url)
            response.raise_for_status()
            
            with open(local_path, 'w', encoding='utf-8') as f:
                f.write(response.text)
            
            downloaded_files.append(local_path)
            print(f"✓ Downloaded {task_name}.json")
            
        except requests.exceptions.RequestException as e:
            print(f"✗ Failed to download {task_name}.json: {e}")
            # Try alternative URL structure
            try:
                alt_url = f"https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/{task_name}/task.json"
                response = requests.get(alt_url)
                response.raise_for_status()
                
                with open(local_path, 'w', encoding='utf-8') as f:
                    f.write(response.text)
                
                downloaded_files.append(local_path)
                print(f"✓ Downloaded {task_name}.json from alternative source")
                
            except requests.exceptions.RequestException as e2:
                print(f"✗ Failed to download {task_name}.json from alternative source: {e2}")
    
    print(f"\nDownloaded {len(downloaded_files)} JSON files to {download_dir}/")
    return downloaded_files

def load_json_to_dataframe(json_file_path):
    """
    Load a BigBench Hard JSON file and convert to DataFrame.
    
    Args:
        json_file_path (str): Path to the JSON file
        
    Returns:
        pandas.DataFrame: DataFrame with 'input' and 'target' columns
    """
    with open(json_file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # Handle different JSON structures
    if 'examples' in data:
        # Standard BIG-bench format
        examples = data['examples']
        df_data = []
        
        for example in examples:
            input_text = example.get('input', '')
            target = example.get('target', '')
            
            # Handle different target formats
            if isinstance(target, list):
                target = target[0] if target else ''
            elif isinstance(target, dict):
                # Handle target_scores format
                if 'target_scores' in example:
                    target_scores = example['target_scores']
                    # Find the target with highest score
                    target = max(target_scores.items(), key=lambda x: x[1])[0]
                else:
                    target = str(target)
            
            df_data.append({
                'input': input_text,
                'target': str(target)
            })
        
        return pd.DataFrame(df_data)
    
    elif isinstance(data, list):
        # Direct list of examples
        df_data = []
        for example in data:
            input_text = example.get('input', '')
            target = example.get('target', example.get('output', ''))
            
            if isinstance(target, list):
                target = target[0] if target else ''
            
            df_data.append({
                'input': input_text,
                'target': str(target)
            })
        
        return pd.DataFrame(df_data)
    
    else:
        raise ValueError(f"Unknown JSON structure in {json_file_path}")

def data_prep_json(json_file_path, num_samples=None):
    """
    Prepare training and test datasets from JSON file.
    
    Args:
        json_file_path (str): Path to the JSON file
        num_samples (int): Number of samples to use (None for all)
        
    Returns:
        tuple: (full_dataset, train_set, test_set, train_targets, test_targets)
    """
    if num_samples is None:
        num_samples = NUM_SAMPLES
    
    # Load JSON data
    dataset_full = load_json_to_dataframe(json_file_path)
    
    # Sample if requested
    if num_samples > 0 and len(dataset_full) > num_samples:
        dataset_sample = dataset_full.sample(num_samples, random_state=42)
    else:
        dataset_sample = dataset_full
    
    # Split into train and test
    train_set_with_targets = dataset_sample.sample(frac=TRAIN_SPLIT_FRACTION, random_state=42)
    test_set_with_targets = dataset_sample.drop(train_set_with_targets.index)
    
    # Save targets separately
    train_targets = train_set_with_targets['target'].copy()
    test_targets = test_set_with_targets['target'].copy()
    
    # Remove target column from datasets
    train_set = train_set_with_targets.drop('target', axis=1).copy()
    test_set = test_set_with_targets.drop('target', axis=1).copy()
    
    # Save to CSV files for compatibility with existing code
    train_set.to_csv("train.csv", index=False)
    test_set.to_csv("test.csv", index=False)
    
    return dataset_sample, train_set, test_set, train_targets, test_targets

def get_available_bbh_tasks(download_dir="bbh-download"):
    """
    Get list of available BigBench Hard tasks from downloaded files.
    
    Args:
        download_dir (str): Directory containing downloaded JSON files
        
    Returns:
        list: List of task names (without .json extension)
    """
    if not os.path.exists(download_dir):
        return []
    
    tasks = []
    for filename in os.listdir(download_dir):
        if filename.endswith('.json'):
            tasks.append(filename[:-5])  # Remove .json extension
    
    return sorted(tasks)

def data_prep(dataset_name):
    """
    Legacy function for backward compatibility with CSV files.
    
    Args:
        dataset_name (str): Name of the dataset (without .csv extension)
        
    Returns:
        tuple: (dataset_sample, train_set, test_set)
    """
    dataset_1000 = pd.read_csv(f"/Users/sriichavali/Desktop/Prompt-Learning/sri/{dataset_name}.csv")
    dataset_50 = dataset_1000.sample(NUM_SAMPLES) if NUM_SAMPLES > 0 else dataset_1000
    train_set = dataset_50.sample(frac=TRAIN_SPLIT_FRACTION, random_state=42)
    test_set = dataset_50.drop(train_set.index)
    train_set.to_csv("train.csv", index=False)
    test_set.to_csv("test.csv", index=False)
    
    return dataset_50, train_set, test_set

"""## Initial System Prompt

Initialize your system prompt. This is the original prompt that will be tested and optimized.
"""

#system_prompt = "You are an expert in JSON webpage creation. This is your task: {input}"
#system_prompt = "You are an expert in solving boolean expressions. Return your answer **in JSON** with a single key `result` whose value is either \"True\" or \"False\". This is your task: {input}"
#system_prompt = "You are an expert in solving truthfulness puzzles. Return your answer **in JSON** with a single key `result` whose value is either \"Yes\" or \"No\". This is your task: {input}"

"""## Evaluator

Here we initialize our evaluator. This uses LLM as a Judge, or using an LLM to evaluate our outputs.  

We will pass in a set of 10 rules to this LLM. It will evaluate each generated JSON against these 10 rules, checking if all are satisfied.

Accordingly, it will give a correctness label, either correct or incorrect.

Additionally, it will attach an explanation as to why it chose correct or incorrect. These explanations will be used to optimize the prompt.
"""

nest_asyncio.apply()

def find_correctness(output):
    """Extract correctness from LLM output"""
    # Look for "correct" or "incorrect" in the response
    pattern = r'"correctness":\s*"?(correct|incorrect)"?'
    match = re.search(pattern, output, re.IGNORECASE)
    if match:
        return match.group(1).lower()
    else:
        return None

def find_explanation(output):
    """Extract explanation from LLM output"""
    # Look for explanation field in JSON
    pattern = r'"explanation":\s*"([^"]*)"'
    match = re.search(pattern, output, re.IGNORECASE)
    if match:
        return match.group(1)
    else:
        return None

def evaluate_output_parser(response: str, row_index: int) -> dict:
    """Parser function for evaluate_output evaluator"""
    correctness = find_correctness(response)
    explanation = find_explanation(response)

    return {
        "correctness": correctness,
        "explanation": explanation
    }


def evaluate_output(dataset, eval_template):
    """Evaluator that checks JSON web page correctness using llm_generate"""

    # Create the evaluation template
    with open(f"{eval_template}.txt", "r") as file:
        evaluation_template = file.read()

    # Create the model
    eval_model = OpenAIModel(
        model="gpt-4o",
        model_kwargs={
            "response_format": {"type": "json_object"},
            "temperature": 0
        }
    )

    # Generate evaluations using llm_generate
    evaluation_results = llm_generate(
        dataframe=dataset,
        template=evaluation_template,
        model=eval_model,
        output_parser=evaluate_output_parser,
        concurrency=20,
        verbose=True
    )

    # Merge the results back into the original dataset
    dataset = dataset.copy()
    for col in ["correctness", "explanation"]:
        if col in evaluation_results.columns:
            dataset[col] = evaluation_results[col]

    return dataset, ["correctness", "explanation"]


def generate_output(dataset, system_prompt):
    output_model = OpenAIModel(
        model="gpt-4.1-2025-04-14",
        model_kwargs={
            "response_format": {"type": "json_object"},
            "temperature": 0
        }
    )
    outputs = llm_generate(
        dataframe=dataset,
        template=system_prompt,
        model=output_model,
        concurrency=20,
        verbose=True
    )
    return outputs["output"]

#test_set.head()

"""## Optimization

There are 3 steps to every loop of optimization.

1. Generate outputs with current prompt on test dataset. Evaluate outputs.
2. If outputs are not satisfactory, generate outputs on training set. Evaluate training outputs.
3. Use training outputs and evaluations to generate optimized prompt.

This process repeats until we see good results on the test set. In this case, we measure our results to be satisfactory when all outputs are deemed "correct" by the evaluate_output evaluator we defined above.
"""

def compute_metric(y_true, y_pred, scorer="accuracy"):
    """
    Compute the requested metric for binary classification.
    y_true and y_pred should be lists or arrays of "correct"/"incorrect" labels.
    scorer: one of "accuracy", "f1", "precision", "recall"
    """
    # Map to binary
    y_true_bin = [1 if y == "correct" else 0 for y in y_true]
    y_pred_bin = [1 if y == "correct" else 0 for y in y_pred]
    if scorer == "accuracy":
        return accuracy_score(y_true_bin, y_pred_bin)
    elif scorer == "f1":
        return f1_score(y_true_bin, y_pred_bin, zero_division=0)
    elif scorer == "precision":
        return precision_score(y_true_bin, y_pred_bin, zero_division=0)
    elif scorer == "recall":
        return recall_score(y_true_bin, y_pred_bin, zero_division=0)
    else:
        raise ValueError(f"Unknown scorer: {scorer}")

def compare_with_targets(outputs, targets, task_type="general"):
    """
    Compare model outputs with ground truth targets.
    
    Args:
        outputs (pd.Series or list): Model outputs (JSON strings)
        targets (pd.Series or list): Ground truth targets
        task_type (str): Type of task for comparison logic
                        - "general": case-insensitive comparison
                        - "sorting": exact case-sensitive comparison (order matters)
                        - "boolean": case-insensitive for True/False
                        - "counting": numeric comparison
        
    Returns:
        float: Accuracy score (0.0 to 1.0)
    """
    correct_count = 0
    total_count = len(outputs)
    
    for output, target in zip(outputs, targets):
        try:
            # Parse JSON output to extract the result
            if isinstance(output, str):
                output_data = json.loads(output)
                predicted_value = output_data.get('result', '')
            else:
                predicted_value = str(output)
            
            # Convert to string for comparison
            predicted_value = str(predicted_value).strip()
            target_value = str(target).strip()
            
            # Task-specific comparison logic
            if task_type == "sorting":
                # For sorting tasks, exact match required (case and order matter)
                if predicted_value == target_value:
                    correct_count += 1
            elif task_type == "counting":
                # For counting tasks, compare as numbers
                try:
                    pred_num = float(predicted_value)
                    target_num = float(target_value)
                    if pred_num == target_num:
                        correct_count += 1
                except ValueError:
                    # If can't convert to numbers, fall back to string comparison
                    if predicted_value.lower() == target_value.lower():
                        correct_count += 1
            else:
                # General case: case-insensitive comparison (for boolean, yes/no, etc.)
                if predicted_value.lower() == target_value.lower():
                    correct_count += 1
                
        except (json.JSONDecodeError, KeyError, AttributeError) as e:
            # If we can't parse the output, count as incorrect
            continue
    
    return correct_count / total_count if total_count > 0 else 0.0

def get_ground_truth_accuracy(outputs, targets, task_type="general"):
    """
    Direct comparison function for outputs vs targets.
    
    Args:
        outputs: Model outputs (can be from results['raw'][i]['output'])
        targets: Ground truth targets
        task_type: Task type for appropriate comparison logic
        
    Returns:
        float: Accuracy score
    """
    return compare_with_targets(outputs, targets, task_type)

def compare_results_with_targets(results, test_targets, train_targets=None, task_type="general"):
    """
    Convenience function to compare all results with ground truth targets.
    
    Args:
        results: Results dictionary from optimize_loop or simple_test
        test_targets: Ground truth test targets
        train_targets: Ground truth train targets (optional)
        task_type: Task type for appropriate comparison logic
        
    Returns:
        dict: Accuracy scores for each iteration and summary
    """
    comparison = {
        'test_accuracies': [],
        'iteration_details': [],
        'task_type': task_type
    }
    
    # Compare test outputs for each iteration
    if 'raw' in results:
        for i, test_df in enumerate(results['raw']):
            if 'output' in test_df.columns:
                accuracy = get_ground_truth_accuracy(test_df['output'], test_targets, task_type)
                comparison['test_accuracies'].append(accuracy)
                
                # Get LLM evaluator score for this iteration if available
                llm_score = results['test'][i] if i < len(results['test']) else None
                
                comparison['iteration_details'].append({
                    'iteration': i,
                    'ground_truth_accuracy': accuracy,
                    'llm_evaluator_score': llm_score,
                    'difference': abs(accuracy - llm_score) if llm_score is not None else None
                })
    
    # Summary stats
    if comparison['test_accuracies']:
        comparison['initial_accuracy'] = comparison['test_accuracies'][0]
        comparison['final_accuracy'] = comparison['test_accuracies'][-1]
        comparison['improvement'] = comparison['final_accuracy'] - comparison['initial_accuracy']
        comparison['best_accuracy'] = max(comparison['test_accuracies'])
    
    return comparison

def analyze_evaluation_comparison(results_df, ground_truth_comparisons):
    """
    Analyze the difference between LLM evaluator scores and ground truth accuracy.
    
    Args:
        results_df (pd.DataFrame): Results dataframe from experiments  
        ground_truth_comparisons (list): List of comparison dicts from compare_results_with_targets()
        
    Returns:
        pd.DataFrame: Comparison analysis
    """
    analysis_data = []
    
    for idx, row in results_df.iterrows():
        if idx < len(ground_truth_comparisons):
            comparison = ground_truth_comparisons[idx]
            
            final_llm_score = row['test'][-1] if isinstance(row['test'], list) else row['test']
            initial_llm_score = row['test'][0] if isinstance(row['test'], list) else row['test']
            
            final_ground_truth_acc = comparison['final_accuracy']
            initial_ground_truth_acc = comparison['initial_accuracy']
            
            file_name = row.get('file', 'unknown')
            task_name = comparison.get('task', f'task_{idx}')
            
            analysis_data.append({
                'experiment': idx,
                'task': task_name,
                'evaluator_template': file_name,
                'initial_llm_score': initial_llm_score,
                'initial_ground_truth_accuracy': initial_ground_truth_acc,
                'final_llm_score': final_llm_score,
                'final_ground_truth_accuracy': final_ground_truth_acc,
                'llm_improvement': final_llm_score - initial_llm_score,
                'ground_truth_improvement': final_ground_truth_acc - initial_ground_truth_acc,
                'final_difference': abs(final_llm_score - final_ground_truth_acc),
                'agreement': 'High' if abs(final_llm_score - final_ground_truth_acc) < 0.1 else 'Low'
            })
    
    return pd.DataFrame(analysis_data)

def optimize_loop(
    train_set,
    test_set,
    system_prompt,
    eval_template,
    evaluators,
    threshold=1,
    loops=NUM_OPTIMIZATION_LOOPS,
    scorer="accuracy"
):
    """
    scorer: one of "accuracy", "f1", "precision", "recall"
    threshold: float, threshold for the selected metric
    num_rules: int, number of rules to use for evaluation (determines which prompt files to load)

    Returns:
        dict with keys:
            "train": list of train set scores per run
            "test": list of test set scores per run
            "prompt": list of system prompts used for each test run
            "raw": list of test set DataFrames (deepcopy) for each test run
            "num_rules": number of rules used for this experiment
    """
    import copy

    curr_loop = 1
    train_metrics = []
    test_metrics = []
    prompts = []
    raw_dfs = []

    print(f"🚀 Starting prompt optimization with {loops} iterations (scorer: {scorer}, threshold: {threshold})")
    print()
    
    # Initial test evaluation before optimization
    print(f"📊 Initial evaluation:")
    test_set["output"] = generate_output(test_set, system_prompt)
    test_evals_all = evaluate_output(test_set, eval_template)[0]
    test_evals = test_evals_all["correctness"]
    y_true = ["correct"] * len(test_evals)
    y_pred = test_evals
    initial_metric_value = compute_metric(y_true, y_pred, scorer=scorer)
    test_metrics.append(initial_metric_value)
    prompts.append(system_prompt)
    raw_dfs.append(copy.deepcopy(test_set))
    
    print(f"✅ Initial test {scorer}: {initial_metric_value}")
    print('\n')
    
    if initial_metric_value >= threshold:
        print(f"🎉 Initial prompt already meets threshold!")
        result = {
            "initial metric": initial_metric_value,
            "train": train_metrics,
            "test": test_metrics,
            "prompt": prompts,
            "file": eval_template,
            "raw": raw_dfs
        }
        return result
    
    while loops > 0:
        print(f"📊 Loop {curr_loop}: Optimizing prompt...")
        
        # 1. Train set evaluation and optimization
        train_outputs = generate_output(train_set, system_prompt)
        train_set["output"] = train_outputs

        train_set["correctness"] = [None] * len(train_set)
        train_set["explanation"] = [None] * len(train_set)
        train_set["rule_violations"] = [None] * len(train_set)

        optimizer = PromptLearningOptimizer(
            prompt=system_prompt,
            model_choice="gpt-4o",
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )

        # Create evaluators with the correct num_rules parameter
        # this is necessary because the evaluators are defined with a default value of NUM_RULES.
        # Your evaluators might be defined differently.
        evaluators_with_rules = []
        for evaluator in evaluators:
            if evaluator.__name__ == 'evaluate_output':
                evaluators_with_rules.append(lambda ds: evaluate_output(ds, eval_template))
            else:
                evaluators_with_rules.append(evaluator)
        
        train_set, _ = optimizer.run_evaluators(
            train_set,
            evaluators_with_rules,
            feedback_columns=["correctness", "explanation", "rule_violations"]
        )

        system_prompt = optimizer.optimize(
            train_set,
            "output",
            feedback_columns=["correctness", "explanation", "rule_violations"],
            context_size_k=128000
        )

        # Evaluate train set after optimization
        train_outputs_post = generate_output(train_set, system_prompt)
        train_set_post = train_set.copy()
        train_set_post["output"] = train_outputs_post
        train_evals_post_all = evaluate_output(train_set_post, eval_template)[0]
        train_evals_post = train_evals_post_all["correctness"]
        y_true_train_post = ["correct"] * len(train_evals_post)
        y_pred_train_post = train_evals_post
        train_metric_post_value = compute_metric(y_true_train_post, y_pred_train_post, scorer=scorer)
        train_metrics.append(train_metric_post_value)
        print(f"✅ Train {scorer}: {train_metric_post_value}")

        # 2. Test set evaluation with optimized prompt
        test_set["output"] = generate_output(test_set, system_prompt)
        
        test_evals_all = evaluate_output(test_set, eval_template)[0]
        test_evals = test_evals_all["correctness"]
        y_true = ["correct"] * len(test_evals)
        y_pred = test_evals
        metric_value = compute_metric(y_true, y_pred, scorer=scorer)
        test_metrics.append(metric_value)
        prompts.append(system_prompt)
        raw_dfs.append(copy.deepcopy(test_set))
        
        print(f"✅ Test {scorer}: {metric_value}")
        print("\n")
        
        # 3. Check threshold
        if metric_value >= threshold:
            print(f"🎉 Threshold reached! Stopping optimization.")
            result = {
                "initial metric": initial_metric_value,
                "train": train_metrics,
                "test": test_metrics,
                "prompt": prompts,
                "file": eval_template,
                "raw": raw_dfs
            }
            return result

        loops -= 1
        curr_loop += 1

    print(f"🔄 All {curr_loop-1} optimization loops completed.")
    result = {
        "initial metric": initial_metric_value,
        "train": train_metrics,
        "test": test_metrics,
        "prompt": prompts,
        "file": eval_template,
        "raw": raw_dfs
    }
    return result

def validate_prompt_files(rule_counts, eval_template):
    """Validate that all required prompt files exist for the given rule counts."""
    import os
    
    missing_files = []
    for num_rules in rule_counts:
        required_files = [
            f"/Users/sriichavali/Desktop/Prompt-Learning/sri/{eval_template}.txt"
        ]
        
        for file_path in required_files:
            if not os.path.exists(file_path):
                missing_files.append(file_path)
    
    if missing_files:
        print("❌ Missing prompt files:")
        for file_path in missing_files:
            print(f"   - {file_path}")
        print("\nAvailable prompt files:")
        for file_path in os.listdir("prompts"):
            print(f"   - prompts/{file_path}")
        raise FileNotFoundError(f"Missing {len(missing_files)} required prompt files")
    
    print("✅ All required prompt files found")

def save_experiment_results(results, filename="experiment_results.json"):
    """Save experiment results to a JSON file."""
    from datetime import datetime
    
    # Add timestamp to results
    results_with_timestamp = {
        "timestamp": datetime.now().isoformat(),
        "results": results
    }
    
    with open(filename, 'w') as f:
        json.dump(results_with_timestamp, f, indent=2, default=str)
    
    print(f"✅ Results saved to {filename}")

def save_single_experiment_csv(results, filename):
    """
    Save a single experiment's results to CSV format.
    
    Args:
        results: Results from a single optimize_loop run
        filename: Output CSV filename
    """
    import pandas as pd
    from datetime import datetime
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Extract data
    #num_rules = results['num_rules']
    train_metrics = results.get('train', [])
    test_metrics = results['test']
    prompts = results['prompt']
    
    # Create DataFrame
    data = []
    for i, (test_metric, prompt) in enumerate(zip(test_metrics, prompts)):
        row = {
            'iteration': i,
            #'num_rules': num_rules,
            'test_accuracy': test_metric,
            'prompt': prompt
        }
        
        # Add train metric if available
        if i < len(train_metrics):
            row['train_accuracy'] = train_metrics[i]
        
        data.append(row)
    
    df = pd.DataFrame(data)
    
    # Save to CSV with timestamp
    filename_with_timestamp = f"{filename}_{timestamp}.csv"
    df.to_csv(filename_with_timestamp, index=False)
    print(f"✅ Results saved to {filename_with_timestamp}")
    #print(f"   📊 {len(df)} iterations, {num_rules} rules")
    print(f"   📈 Final accuracy: {df['test_accuracy'].iloc[-1]:.3f}")

def save_multi_experiment_csv(results, base_filename="experiment_results"):
    """
    Save multiple experiments' results to separate CSV files.
    
    Args:
        results: Results from run_multi_rule_experiments
        base_filename: Base name for the CSV files
    """
    from datetime import datetime
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    for experiment_name, experiment_results in results.items():
        csv_filename = f"{base_filename}_{experiment_name}_{timestamp}.csv"
        save_single_experiment_csv(experiment_results, csv_filename)

def simple_test(train_set, test_set, system_prompt, eval_template, results_df, threshold=1, loops=NUM_OPTIMIZATION_LOOPS, scorer="accuracy"):
    """
    Run prompt optimization experiment with LLM evaluator only.
    For ground truth comparison, use compare_results_with_targets() afterwards.
    
    Args:
        loops: Number of optimization loops (defaults to NUM_OPTIMIZATION_LOOPS config)
    """
    evaluators = [evaluate_output]
    results = optimize_loop(
        train_set, test_set, system_prompt, eval_template, evaluators,
        threshold=threshold,
        loops=loops,
        scorer=scorer
    )
    print("✅ Completed experiment")
    print(f"   Initial metric: {results['initial metric']:.3f}")
    print(f"   Final test {scorer}: {results['test'][-1]:.3f}")
    print(f"   Final prompt: {results['prompt'][-1]}")
    
    results_df = pd.concat([results_df, pd.DataFrame({k: [v] for k, v in results.items()})], ignore_index=True)
    return results, results_df

wol_prompt = "You are an expert in solving truthfulness puzzles. Return your answer **in JSON** with a single key `result` whose value is either \"Yes\" or \"No\". This is your task: {input}"
bool_prompt = "You are an expert in solving boolean expressions. Return your answer **in JSON** with a single key `result` whose value is either \"True\" or \"False\". This is your task: {input}"
word_sorting_prompt = "You are an expert in sorting words alphabetically. Return your answer **in JSON** with a single key `result` whose value is the alphabetically sorted list of words separated by spaces. This is your task: {input}"
sports_prompt = "You are an expert in understanding sports. Return your answer **in JSON** with a single key `result` whose value is either \"Yes\" or \"No\". This is your task: {input}"
object_prompt = "You are an expert in counting objects. Return your answer **in JSON** with a single key `result` whose value is the number of objects in the input. This is your task: {input}"

# Main execution code - uncomment to run experiments with CSV data
# Note: This code assumes you have the original CSV files. 
# For BigBench Hard JSON experiments, use run_bbh_experiments() instead.

"""
# Clean approach - separate optimization from ground truth comparison
columns = ["initial metric", "train", "test", "prompt", "file", "raw"]
result_df = pd.DataFrame(columns=columns)

dataset_50, train_set, test_set, train_targets, test_targets = data_prep_json("bbh-download/web_of_lies.json")
system_prompt = wol_prompt
eval_template = "evaluator-lies"
results, result_df = simple_test(train_set, test_set, system_prompt, eval_template, result_df)
# Ground truth comparison separately
comparison = compare_results_with_targets(results, test_targets)
print(f"Ground truth improvement: {comparison['improvement']:.3f}")

dataset_50, train_set, test_set, train_targets, test_targets = data_prep_json("bbh-download/boolean_expressions.json")
system_prompt = bool_prompt
eval_template = "evaluator-bool"
results, result_df = simple_test(train_set, test_set, system_prompt, eval_template, result_df)
comparison = compare_results_with_targets(results, test_targets)

dataset_50, train_set, test_set, train_targets, test_targets = data_prep_json("bbh-download/word_sorting.json")
system_prompt = word_sorting_prompt
eval_template = "evaluator-wordsort"
results, result_df = simple_test(train_set, test_set, system_prompt, eval_template, result_df)
comparison = compare_results_with_targets(results, test_targets)

dataset_50, train_set, test_set, train_targets, test_targets = data_prep_json("bbh-download/sports_understanding.json")
system_prompt = sports_prompt
eval_template = "evaluator-sports"
results, result_df = simple_test(train_set, test_set, system_prompt, eval_template, result_df)
comparison = compare_results_with_targets(results, test_targets)

dataset_50, train_set, test_set, train_targets, test_targets = data_prep_json("bbh-download/object_counting.json")
system_prompt = object_prompt
eval_template = "evaluator-object"
results, result_df = simple_test(train_set, test_set, system_prompt, eval_template, result_df)
comparison = compare_results_with_targets(results, test_targets)

result_df.to_csv("results.csv")
"""

"""## Example: Using BigBench Hard JSON Data

Here's how to use the new JSON-based functionality:

```python
# 1. Download all BigBench Hard JSON files
downloaded_files = download_bbh_json_files("bbh-download")

# 2. Get list of available tasks
available_tasks = get_available_bbh_tasks("bbh-download")
print("Available tasks:", available_tasks)

# 3. Load a specific task (note: targets are now separated)
task_name = "boolean_expressions"
dataset, train_set, test_set, train_targets, test_targets = data_prep_json(f"bbh-download/{task_name}.json")

# 4. Run optimization experiment (clean, no targets)
system_prompt = bool_prompt
eval_template = "evaluator-bool"
results, result_df = simple_test(train_set, test_set, system_prompt, eval_template, result_df)

# 5. Compare with ground truth separately (specify task type for accurate comparison)
comparison = compare_results_with_targets(results, test_targets, task_type="boolean")  # Use "sorting", "counting", etc. as needed
print(f"Ground truth improvement: {comparison['improvement']:.3f}")
```

Example usage for multiple tasks:
```python
# Download files first
download_bbh_json_files()

# Define task mappings
task_mappings = {
    "boolean_expressions": ("evaluator-bool", bool_prompt),
    "web_of_lies": ("evaluator-lies", wol_prompt),
    "word_sorting": ("evaluator-wordsort", word_sorting_prompt),
    "sports_understanding": ("evaluator-sports", sports_prompt),
    "object_counting": ("evaluator-object", object_prompt),
}

# Run experiments for each task
for task_name, (eval_template, prompt) in task_mappings.items():
    print(f"Running experiment for {task_name}...")
    dataset, train_set, test_set, train_targets, test_targets = data_prep_json(f"bbh-download/{task_name}.json")
    
    # Run optimization (clean)
    results, result_df = simple_test(train_set, test_set, prompt, eval_template, result_df)
    
    # Ground truth comparison
    comparison = compare_results_with_targets(results, test_targets)
    print(f"  Ground truth accuracy: {comparison['final_accuracy']:.3f} (improved {comparison['improvement']:.3f})")
```

Key changes:
- Target columns are automatically removed from train/test sets
- Ground truth targets are saved separately for comparison
- Results now include ground truth accuracy for each iteration
- LLM evaluator scores are separate from ground truth accuracy scores

Direct comparison examples (CLEAN APPROACH):
```python
# Load data (targets separated automatically)
dataset, train_set, test_set, train_targets, test_targets = data_prep_json("bbh-download/boolean_expressions.json")

# Run optimization experiment (clean - no targets passed)
results, results_df = simple_test(train_set, test_set, bool_prompt, "evaluator-bool", results_df)

# THEN do ground truth comparison separately:

# Method 1: Quick comparison (with task type for accurate comparison)
comparison = compare_results_with_targets(results, test_targets, task_type="sorting")  # Use appropriate task type
print(f"Ground truth improvement: {comparison['improvement']:.3f}")
print(f"Final accuracy: {comparison['final_accuracy']:.3f}")

# Method 2: Manual comparison for specific iteration
initial_accuracy = get_ground_truth_accuracy(results['raw'][0]['output'], test_targets, "sorting")
final_accuracy = get_ground_truth_accuracy(results['raw'][-1]['output'], test_targets, "sorting")
print(f"Initial: {initial_accuracy:.3f} → Final: {final_accuracy:.3f}")

# Method 3: Detailed analysis 
for detail in comparison['iteration_details']:
    print(f"Iter {detail['iteration']}: GT={detail['ground_truth_accuracy']:.3f}, LLM={detail['llm_evaluator_score']:.3f}")
```
"""

# Uncomment the following lines to download BigBench Hard JSON files and switch to JSON-based data loading
"""
# Download BigBench Hard JSON files
print("Downloading BigBench Hard JSON files...")
downloaded_files = download_bbh_json_files("bbh-download")

# Show available tasks
available_tasks = get_available_bbh_tasks("bbh-download")
print(f"Available tasks: {available_tasks}")

# Example: Load boolean expressions task
if "boolean_expressions" in available_tasks:
    print("\nExample: Loading boolean_expressions task...")
    dataset, train_set, test_set = data_prep_json("bbh-download/boolean_expressions.json")
    print(f"Dataset shape: {dataset.shape}")
    print(f"Train set shape: {train_set.shape}")
    print(f"Test set shape: {test_set.shape}")
    print(f"Sample input: {dataset.iloc[0]['input']}")
    print(f"Sample target: {dataset.iloc[0]['target']}")
"""

def run_bbh_experiments():
    """
    Function to run experiments on BigBench Hard tasks.
    Call this function to download JSON files and run experiments.
    """
    # Download files if not already downloaded
    if not os.path.exists("bbh-download") or len(get_available_bbh_tasks("bbh-download")) == 0:
        print("Downloading BigBench Hard JSON files...")
        downloaded_files = download_bbh_json_files("bbh-download")
    
    # Define task mappings (evaluator, prompt, task_type)
    task_mappings = {
        "boolean_expressions": ("evaluator-bool", bool_prompt, "boolean"),
        "web_of_lies": ("evaluator-lies", wol_prompt, "general"),
        "word_sorting": ("evaluator-wordsort", word_sorting_prompt, "sorting"),
        "sports_understanding": ("evaluator-sports", sports_prompt, "general"),
        "object_counting": ("evaluator-object", object_prompt, "counting"),
    }
    
    results_df = pd.DataFrame(columns=["initial metric", "train", "test", "prompt", "file", "raw"])
    all_comparisons = []  # Store ground truth comparisons separately
    
    # Run experiments for each task
    for task_name, (eval_template, prompt, task_type) in task_mappings.items():
        json_file_path = f"bbh-download/{task_name}.json"
        if os.path.exists(json_file_path):
            print(f"\n🔬 Running experiment for {task_name}...")
            dataset, train_set, test_set, train_targets, test_targets = data_prep_json(json_file_path)
            
            # Run the optimization experiment (clean, no targets)
            results, results_df = simple_test(train_set, test_set, prompt, eval_template, results_df)
            
            # Do ground truth comparison separately with appropriate task type
            print(f"📊 Comparing with ground truth (task_type: {task_type})...")
            comparison = compare_results_with_targets(results, test_targets, task_type=task_type)
            comparison['task'] = task_name
            comparison['eval_template'] = eval_template
            all_comparisons.append(comparison)
            
            # Print ground truth results
            print(f"   Initial ground truth accuracy: {comparison['initial_accuracy']:.3f}")
            print(f"   Final ground truth accuracy: {comparison['final_accuracy']:.3f}")
            print(f"   Ground truth improvement: {comparison['improvement']:.3f}")
            
        else:
            print(f"⚠️  Skipping {task_name} - file not found")
    
    # Save results
    results_df.to_csv("bbh_results.csv")
    print("\n✅ LLM evaluator results saved to bbh_results.csv")
    
    # Save ground truth comparison results
    if all_comparisons:
        comparison_df = pd.DataFrame([
            {
                'task': comp['task'],
                'eval_template': comp['eval_template'],
                'initial_gt_accuracy': comp['initial_accuracy'],
                'final_gt_accuracy': comp['final_accuracy'],
                'gt_improvement': comp['improvement'],
                'best_gt_accuracy': comp['best_accuracy']
            }
            for comp in all_comparisons
        ])
        comparison_df.to_csv("bbh_ground_truth_comparison.csv", index=False)
        print("✅ Ground truth comparison saved to bbh_ground_truth_comparison.csv")
        
        # Create summary table with LLM evaluator scores
        summary_data = []
        for i, (comp, (idx, row)) in enumerate(zip(all_comparisons, results_df.iterrows())):
            final_llm_score = row['test'][-1] if isinstance(row['test'], list) and len(row['test']) > 0 else 0.0
            initial_llm_score = row['test'][0] if isinstance(row['test'], list) and len(row['test']) > 0 else 0.0
            
            summary_data.append({
                'Task': comp['task'],
                'Initial_Ground_Truth_Accuracy': comp['initial_accuracy'],
                'Final_Ground_Truth_Accuracy': comp['final_accuracy'],
                'Initial_LLM_Evaluator_Score': initial_llm_score,
                'Final_LLM_Evaluator_Score': final_llm_score,
                'Ground_Truth_Improvement': comp['improvement'],
                'LLM_Evaluator_Improvement': final_llm_score - initial_llm_score,
                'Task_Type': comp['task_type']
            })
        
        summary_df = pd.DataFrame(summary_data)
        
        # Print formatted table
        print(f"\n📊 EXPERIMENT SUMMARY TABLE")
        print("=" * 100)
        print(f"{'Task':<20} {'Init GT':<8} {'Final GT':<9} {'Init LLM':<9} {'Final LLM':<10} {'GT Δ':<7} {'LLM Δ':<8} {'Type':<8}")
        print("-" * 100)
        
        for _, row in summary_df.iterrows():
            print(f"{row['Task']:<20} {row['Initial_Ground_Truth_Accuracy']:<8.3f} "
                  f"{row['Final_Ground_Truth_Accuracy']:<9.3f} {row['Initial_LLM_Evaluator_Score']:<9.3f} "
                  f"{row['Final_LLM_Evaluator_Score']:<10.3f} {row['Ground_Truth_Improvement']:<7.3f} "
                  f"{row['LLM_Evaluator_Improvement']:<8.3f} {row['Task_Type']:<8}")
        
        print("-" * 100)
        print(f"{'AVERAGE':<20} {summary_df['Initial_Ground_Truth_Accuracy'].mean():<8.3f} "
              f"{summary_df['Final_Ground_Truth_Accuracy'].mean():<9.3f} "
              f"{summary_df['Initial_LLM_Evaluator_Score'].mean():<9.3f} "
              f"{summary_df['Final_LLM_Evaluator_Score'].mean():<10.3f} "
              f"{summary_df['Ground_Truth_Improvement'].mean():<7.3f} "
              f"{summary_df['LLM_Evaluator_Improvement'].mean():<8.3f}")
        
        # Save summary table (not in .gitignore)
        summary_df.to_csv("experiment_summary_table.txt", index=False, sep='\t')
        print(f"\n✅ Summary table saved to experiment_summary_table.txt")
        
        # Save detailed raw comparison data (not in .gitignore)
        import os
        os.makedirs("raw_comparison_data", exist_ok=True)
        
        for i, comp in enumerate(all_comparisons):
            task_name = comp['task']
            
            # Save iteration details for this task
            details_df = pd.DataFrame(comp['iteration_details'])
            details_filename = f"raw_comparison_data/{task_name}_iteration_details.txt"
            details_df.to_csv(details_filename, index=False, sep='\t')
            
            # Save test accuracies for this task
            accuracies_data = {
                'iteration': list(range(len(comp['test_accuracies']))),
                'ground_truth_accuracy': comp['test_accuracies'],
                'task_type': [comp['task_type']] * len(comp['test_accuracies'])
            }
            accuracies_df = pd.DataFrame(accuracies_data)
            accuracies_filename = f"raw_comparison_data/{task_name}_ground_truth_accuracies.txt"
            accuracies_df.to_csv(accuracies_filename, index=False, sep='\t')
        
        print(f"✅ Raw comparison data saved to raw_comparison_data/ directory")
        print(f"   📁 Files: {len(all_comparisons) * 2} detailed files (iteration details + accuracies per task)")
        
        # Print final summary
        print(f"\n📈 Overall Results:")
        print(f"   Average final ground truth accuracy: {summary_df['Final_Ground_Truth_Accuracy'].mean():.3f}")
        print(f"   Average ground truth improvement: {summary_df['Ground_Truth_Improvement'].mean():.3f}")
        print(f"   Average final LLM evaluator score: {summary_df['Final_LLM_Evaluator_Score'].mean():.3f}")
        print(f"   Average LLM evaluator improvement: {summary_df['LLM_Evaluator_Improvement'].mean():.3f}")
        print(f"   Best performing task (GT): {summary_df.loc[summary_df['Final_Ground_Truth_Accuracy'].idxmax(), 'Task']}")
        print(f"   Best performing task (LLM): {summary_df.loc[summary_df['Final_LLM_Evaluator_Score'].idxmax(), 'Task']}")
    
    return results_df, all_comparisons, summary_df

# To run BigBench Hard experiments, uncomment the following line:
# bbh_results = run_bbh_experiments()

# =============================================================================
# GETTING STARTED
# =============================================================================
print("🚀 BigBench Hard JSON Integration Loaded!")
print("📖 For usage instructions, see README_BBH.md")
print("")
print("⚙️  Configuration: Edit NUM_OPTIMIZATION_LOOPS at top of file to control experiment iterations")
print("")
print("Quick start:")
print("1. Test download: python test_bbh_download.py")
print("2. Run experiments: from pl_multidataset import run_bbh_experiments; run_bbh_experiments()")
print("3. Or download files manually: from pl_multidataset import download_bbh_json_files; download_bbh_json_files()")
print("")
print("Available functions:")
print("- download_bbh_json_files() - Download BigBench Hard JSON files")
print("- get_available_bbh_tasks() - List available tasks") 
print("- data_prep_json() - Prepare data from JSON files (targets removed & saved separately)")
print("- run_bbh_experiments() - Run complete experiments")
print("- compare_with_targets() - Compare model outputs with ground truth")
print("- get_ground_truth_accuracy() - Direct comparison: outputs vs targets")
print("- compare_results_with_targets() - Convenience function for full results comparison")
print("- analyze_evaluation_comparison() - Compare LLM evaluator vs ground truth scores")
print("")
print("🆕 NEW: Clean separation of concerns")
print("     1. Target columns automatically removed from train/test sets") 
print("     2. simple_test() does LLM evaluation only (clean)")
print("     3. compare_results_with_targets() does ground truth comparison separately")
print("     4. Best of both worlds: LLM evaluator scores + objective accuracy")
print("")
print("To run the original CSV-based experiments, uncomment the code block above.")
print("=" * 80)