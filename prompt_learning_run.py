# -*- coding: utf-8 -*-
"""prompt_learning_cookbook_AX.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IhN8oZJOxkft9eLY8pJte5nWdFlbl0C4

[![My Image](data:image/png;base64,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)](https://arize.com)

# Optimizing JSON Webpage Prompts with the Arize Prompt Learning SDK

In this cookbook, we demonstrate a use case of the Arize Prompt Learning SDK by optimizing a system prompt for GPT-4.1. The goal is to improve the modelâ€™s ability to generate accurate JSON representations of webpages in response to user queries. The dataset consists of prompts asking GPT to generate webpages, and we define 10 specific rules that the JSON outputs must satisfy. Using the SDK, we iteratively refine the prompt to achieve high accuracy on the training set, and then evaluate its performance on a separate test set.
"""

#!pip install arize-phoenix-evals arize-phoenix-client tiktoken openai arize-toolkit

# CONFIG: Number of samples to use for the experiment. Adjust as needed.
NUM_SAMPLES = 40  # Number of rows to sample from the full dataset, 0 for all
TRAIN_SPLIT_FRACTION = 0.5  # Fraction of data to use for training (rest for testing)
NUM_RULES = 50  # Number of rules in the prompt - adjust based on your evaluator prompt (this is NOT working on Config)

import nest_asyncio, re
nest_asyncio.apply()

# 1ï¸âƒ£  stricter variable detector
from phoenix.evals.templates import PromptTemplate, PromptPartTemplate
_TEMPLATE_RE = re.compile(r"\{([a-zA-Z_][a-zA-Z0-9_]*)\}")
def _parse_variables_strict(self, tmpl: list[PromptPartTemplate]):  # [...]
    vars = set()
    for p in tmpl:
        vars.update(m.group(1) for m in _TEMPLATE_RE.finditer(p.template))
    return list(vars)
PromptTemplate._parse_variables = _parse_variables_strict

# 2ï¸âƒ£  literalâ€‘brace formatter
from phoenix.evals.templates import PromptPart, MultimodalPrompt
def _format_literal(self, variable_values, options=None):  # [...]
    prompt_msgs = []
    for part in self.prompt(options):
        msg = part.template
        for var in self.variables:
            msg = msg.replace(f"{{{var}}}", str(variable_values[var]))
        prompt_msgs.append(PromptPart(content_type=part.content_type, content=msg))
    return MultimodalPrompt(parts=prompt_msgs)
PromptTemplate.format = _format_literal

"""## OpenAI Key
We will be using OpenAI to generate the webpage jsons.
"""

import os, getpass
import openai
client = openai.Client(api_key=os.getenv("OPENAI_API_KEY"))

"""## Training and Test Datasets

Create training and test datasets, and export to Arize.

First, download the [dataset of queries](https://storage.googleapis.com/arize-assets/dev-rel/prompt-learning/queries.csv).
"""

import pandas as pd

dataset_1000 = pd.read_csv("https://storage.googleapis.com/arize-assets/dev-rel/prompt-learning/queries.csv")


dataset_50 = dataset_1000.sample(NUM_SAMPLES) if NUM_SAMPLES > 0 else dataset_1000

# 80-20 split
train_set = dataset_50.sample(frac=TRAIN_SPLIT_FRACTION, random_state=42)
test_set = dataset_50.drop(train_set.index)

train_set.to_csv("train.csv", index=False)
test_set.to_csv("test.csv", index=False)

"""## Initial System Prompt

Initialize your system prompt. This is the original prompt that will be tested and optimized.
"""

system_prompt = "You are an expert in JSON webpage creation. This is your task: {input}"

"""## Evaluator

Here we initialize our evaluator. This uses LLM as a Judge, or using an LLM to evaluate our outputs.  

We will pass in a set of 10 rules to this LLM. It will evaluate each generated JSON against these 10 rules, checking if all are satisfied.

Accordingly, it will give a correctness label, either correct or incorrect.

Additionally, it will attach an explanation as to why it chose correct or incorrect. These explanations will be used to optimize the prompt.
"""

import re
import json
from phoenix.evals import OpenAIModel, llm_generate
import nest_asyncio
nest_asyncio.apply()

def find_correctness(output):
    """Extract correctness from LLM output"""
    # Look for "correct" or "incorrect" in the response
    pattern = r'"correctness":\s*"?(correct|incorrect)"?'
    match = re.search(pattern, output, re.IGNORECASE)
    if match:
        return match.group(1).lower()
    else:
        return None

def find_explanation(output):
    """Extract explanation from LLM output"""
    # Look for explanation field in JSON
    pattern = r'"explanation":\s*"([^"]*)"'
    match = re.search(pattern, output, re.IGNORECASE)
    if match:
        return match.group(1)
    else:
        return None

def evaluate_output_parser(response: str, row_index: int) -> dict:
    """Parser function for evaluate_output evaluator"""
    correctness = find_correctness(response)
    explanation = find_explanation(response)

    return {
        "correctness": correctness,
        "explanation": explanation
    }

def rule_checker_parser(response: str, row_index: int) -> dict:
    """Parser function for rule_checker evaluator"""
    explanation = find_explanation(response)

    return {
        "rule_violations": explanation
    }

def evaluate_output(dataset):
    """Evaluator that checks JSON web page correctness using llm_generate"""

    # Create the evaluation template

    with open("evaluator-prompt-50.txt", "r") as file:
        evaluation_template = file.read()

    # Create the model
    eval_model = OpenAIModel(
        model="gpt-4o",
        model_kwargs={
            "response_format": {"type": "json_object"},
            "temperature": 0
        }
    )

    # Generate evaluations using llm_generate
    evaluation_results = llm_generate(
        dataframe=dataset,
        template=evaluation_template,
        model=eval_model,
        output_parser=evaluate_output_parser,
        concurrency=20,
        verbose=True
    )

    # Merge the results back into the original dataset
    dataset = dataset.copy()
    for col in ["correctness", "explanation"]:
        if col in evaluation_results.columns:
            dataset[col] = evaluation_results[col]

    return dataset, ["correctness", "explanation"]

def rule_checker(dataset):
    """Evaluator that checks which rules are broken using llm_generate"""

    # Create the rule checking template
    with open("rule-checker-prompt-50.txt", "r") as file:
        rule_check_template = file.read()

    # Create the model
    eval_model = OpenAIModel(
        model="gpt-4o",
        model_kwargs={
            "response_format": {"type": "json_object"},
            "temperature": 0
        }
    )

    # Generate rule checks using llm_generate
    rule_check_results = llm_generate(
        dataframe=dataset,
        template=rule_check_template,
        model=eval_model,
        output_parser=rule_checker_parser,
        concurrency=40,
        verbose=True
    )

    # Merge the results back into the original dataset
    dataset = dataset.copy()
    if "rule_violations" in rule_check_results.columns:
        dataset["rule_violations"] = rule_check_results["rule_violations"]

    return dataset, ["rule_violations"]

def rules_followed_metric(dataset):
    with open("metrics-prompt-50.txt", "r") as file:
        metric_prompt = file.read()

    eval_model = OpenAIModel(
        model="gpt-4.1-2025-04-14",
        model_kwargs={
            "temperature": 0
        }
    )

    rule_followed_results = llm_generate(
        dataframe=dataset,
        template=metric_prompt,
        model=eval_model,
        output_parser=rule_checker_parser,
        concurrency=40,
        verbose=True
    )

    results = []
    return rule_followed_results

from phoenix.evals import llm_generate

def generate_output(dataset, system_prompt):
    output_model = OpenAIModel(
        model="gpt-4.1-2025-04-14",
        model_kwargs={
            "response_format": {"type": "json_object"},
            "temperature": 0
        }
    )
    outputs = llm_generate(
        dataframe=dataset,
        template=system_prompt,
        model=output_model,
        concurrency=40,
        verbose=True
    )
    return outputs["output"]

test_set.head()

"""## Optimization

There are 3 steps to every loop of optimization.

1. Generate outputs with current prompt on test dataset. Evaluate outputs.
2. If outputs are not satisfactory, generate outputs on training set. Evaluate training outputs.
3. Use training outputs and evaluations to generate optimized prompt.

This process repeats until we see good results on the test set. In this case, we measure our results to be satisfactory when all outputs are deemed "correct" by the evaluate_output evaluator we defined above.
"""

from meta_prompt_optimizer import MetaPromptOptimizer

num_rules = NUM_RULES  # Use config value from top of file

from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score

def compute_metric(y_true, y_pred, scorer="accuracy"):
    """
    Compute the requested metric for binary classification.
    y_true and y_pred should be lists or arrays of "correct"/"incorrect" labels.
    scorer: one of "accuracy", "f1", "precision", "recall"
    """
    # Map to binary
    y_true_bin = [1 if y == "correct" else 0 for y in y_true]
    y_pred_bin = [1 if y == "correct" else 0 for y in y_pred]
    if scorer == "accuracy":
        return accuracy_score(y_true_bin, y_pred_bin)
    elif scorer == "f1":
        return f1_score(y_true_bin, y_pred_bin, zero_division=0)
    elif scorer == "precision":
        return precision_score(y_true_bin, y_pred_bin, zero_division=0)
    elif scorer == "recall":
        return recall_score(y_true_bin, y_pred_bin, zero_division=0)
    else:
        raise ValueError(f"Unknown scorer: {scorer}")

def optimize_loop(
    train_set,
    test_set,
    system_prompt,
    evaluators,
    threshold=0.7,
    loops=5,
    scorer="accuracy"
):
    """
    scorer: one of "accuracy", "f1", "precision", "recall"
    threshold: float, threshold for the selected metric

    Returns:
        dict with keys:
            "train": list of train set scores per run
            "test": list of test set scores per run
            "prompt": list of system prompts used for each test run
            "raw": list of test set DataFrames (deepcopy) for each test run
    """
    import copy

    curr_loop = 1
    train_metrics = []
    test_metrics = []
    prompts = []
    raw_dfs = []

    print(f"ðŸš€ Starting prompt optimization with {loops} iterations (scorer: {scorer}, threshold: {threshold})")
    while loops > 0:
        print(f"\nðŸ“Š Loop {curr_loop}: Testing current prompt...")
        # Test set evaluation
        test_set["output"] = generate_output(test_set, system_prompt)

        test_evals_all = evaluate_output(test_set)[0]
        test_evals = test_evals_all["correctness"]
        y_true = ["correct"] * len(test_evals)
        y_pred = test_evals

        metric_value = compute_metric(y_true, y_pred, scorer=scorer)

        # Store test metrics for this run (only score)
        test_metrics.append(metric_value)
        prompts.append(system_prompt)
        raw_dfs.append(copy.deepcopy(test_set))

      
        print(f"âœ… Test {scorer}: {metric_value}")
        print("\n")
        if metric_value >= threshold:
            return {
                "train": train_metrics,
                "test": test_metrics,
                "prompt": prompts,
                "raw": raw_dfs
            }

        else:
            print(f"ðŸ”„ {len(test_evals) - sum(1 for item in test_evals if item == 'correct')} test queries failed. Optimizing prompt...")
            # Train set evaluation
            train_outputs = generate_output(train_set, system_prompt)
            train_set["output"] = train_outputs

            train_set["correctness"] = [None] * len(train_set)
            train_set["explanation"] = [None] * len(train_set)
            train_set["rule_violations"] = [None] * len(train_set)

            optimizer = MetaPromptOptimizer(
                prompt=system_prompt,
                model_choice="gpt-4.1-2025-04-14",
                openai_api_key=os.getenv("OPENAI_API_KEY")
            )

            train_set, _ = optimizer.run_evaluators(
                train_set,
                evaluators,
                feedback_columns=["correctness", "explanation", "rule_violations"]
            )

            # Evaluate train set after optimization
            train_outputs_post = generate_output(train_set, system_prompt)
            train_set_post = train_set.copy()
            train_set_post["output"] = train_outputs_post
            train_evals_post_all = evaluate_output(train_set_post)[0]
            train_evals_post = train_evals_post_all["correctness"]
            y_true_train_post = ["correct"] * len(train_evals_post)
            y_pred_train_post = train_evals_post
            train_metric_post_value = compute_metric(y_true_train_post, y_pred_train_post, scorer=scorer)
            train_metrics.append(train_metric_post_value)
            print(f"âœ… Train {scorer}: {train_metric_post_value}")

            system_prompt = optimizer.optimize(
                train_set,
                "output",
                feedback_columns=["correctness", "explanation", "rule_violations"],
                context_size_k=128000
            )

            loops -= 1
            curr_loop += 1

    # Final evaluation after all loops
    test_outputs = generate_output(test_set, system_prompt)
    test_set["output"] = test_outputs
    test_evals_all = evaluate_output(test_set)[0]
    test_evals = test_evals_all["correctness"]
    y_true = ["correct"] * len(test_evals)
    y_pred = test_evals
    metric_value = compute_metric(y_true, y_pred, scorer=scorer)
    test_metrics.append(metric_value)
    prompts.append(system_prompt)
    raw_dfs.append(copy.deepcopy(test_set))

    # Final train evaluation with optimized prompt
    train_outputs_final = generate_output(train_set, system_prompt)
    train_set_final = train_set.copy()
    train_set_final["output"] = train_outputs_final
    train_evals_final_all = evaluate_output(train_set_final)[0]
    train_evals_final = train_evals_final_all["correctness"]
    y_true_train_final = ["correct"] * len(train_evals_final)
    y_pred_train_final = train_evals_final
    train_metric_final_value = compute_metric(y_true_train_final, y_pred_train_final, scorer=scorer)
    train_metrics.append(train_metric_final_value)

    print(f"âœ… Final test {scorer}: {metric_value} ({sum(1 for item in test_evals if item == 'correct')}/{len(test_evals)} correct)")
    print(f"âœ… Final train {scorer}: {train_metric_final_value} ({sum(1 for item in train_evals_final if item == 'correct')}/{len(train_evals_final)} correct)")
    return {
        "train": train_metrics,
        "test": test_metrics,
        "prompt": prompts,
        "raw": raw_dfs
    }

evaluators = [evaluate_output, rule_checker]
results = optimize_loop(
    train_set, test_set, system_prompt, evaluators
)

"""## Now you have your optimized system prompt!"""

print(results)

