# -*- coding: utf-8 -*-
"""prompt_learning_cookbook_AX.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IhN8oZJOxkft9eLY8pJte5nWdFlbl0C4

[![My Image](data:image/png;base64,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)](https://arize.com)

# Optimizing JSON Webpage Prompts with the Arize Prompt Learning SDK

In this cookbook, we demonstrate a use case of the Arize Prompt Learning SDK by optimizing a system prompt for GPT-4.1. The goal is to improve the model’s ability to generate accurate JSON representations of webpages in response to user queries. The dataset consists of prompts asking GPT to generate webpages, and we define 10 specific rules that the JSON outputs must satisfy. Using the SDK, we iteratively refine the prompt to achieve high accuracy on the training set, and then evaluate its performance on a separate test set.
"""

#!pip install arize-phoenix-evals arize-phoenix-client tiktoken openai arize-toolkit

# CONFIG: Number of samples to use for the experiment. Adjust as needed.
NUM_SAMPLES = 100  # Number of rows to sample from the full dataset, 0 for all
TRAIN_SPLIT_FRACTION = 0.5  # Fraction of data to use for training (rest for testing)
NUM_RULES = 50  # Number of rules in the prompt - adjust based on your evaluator prompt (this is NOT working on Config)

# EXPERIMENT CONFIGURATION
RUN_MULTI_RULE_EXPERIMENTS = False  # Set to True to run experiments with multiple rule counts
RULE_COUNTS_TO_TEST = [10, 50, 100]  # Rule counts to test in multi-rule experiments
NUM_OPTIMIZATION_LOOPS = 5  # Number of optimization loops per experiment

# USAGE EXAMPLES:
# 1. Single experiment with 50 rules (default):
#    - Set RUN_MULTI_RULE_EXPERIMENTS = False
#    - Results saved to "single_experiment_results.json"
#
# 2. Multi-rule experiments:
#    - Set RUN_MULTI_RULE_EXPERIMENTS = True
#    - Adjust RULE_COUNTS_TO_TEST as needed
#    - Results saved to "multi_rule_experiments.json"
#
# 3. Load previous results:
#    - Use load_experiment_results("filename.json")
#
# 4. Control optimization loops:
#    - Set NUM_OPTIMIZATION_LOOPS to control how many iterations per experiment

import nest_asyncio, re
nest_asyncio.apply()

# 1️⃣  stricter variable detector
from phoenix.evals.templates import PromptTemplate, PromptPartTemplate
_TEMPLATE_RE = re.compile(r"\{([a-zA-Z_][a-zA-Z0-9_]*)\}")
def _parse_variables_strict(self, tmpl: list[PromptPartTemplate]):  # [...]
    vars = set()
    for p in tmpl:
        vars.update(m.group(1) for m in _TEMPLATE_RE.finditer(p.template))
    return list(vars)
PromptTemplate._parse_variables = _parse_variables_strict

# 2️⃣  literal‑brace formatter
from phoenix.evals.templates import PromptPart, MultimodalPrompt
def _format_literal(self, variable_values, options=None):  # [...]
    prompt_msgs = []
    for part in self.prompt(options):
        msg = part.template
        for var in self.variables:
            msg = msg.replace(f"{{{var}}}", str(variable_values[var]))
        prompt_msgs.append(PromptPart(content_type=part.content_type, content=msg))
    return MultimodalPrompt(parts=prompt_msgs)
PromptTemplate.format = _format_literal

"""## OpenAI Key
We will be using OpenAI to generate the webpage jsons.
"""

import os, getpass
import openai
client = openai.Client(api_key=os.getenv("OPENAI_API_KEY"))

"""## Training and Test Datasets

Create training and test datasets, and export to Arize.

First, download the [dataset of queries](https://storage.googleapis.com/arize-assets/dev-rel/prompt-learning/queries.csv).
"""

import pandas as pd

dataset_1000 = pd.read_csv("https://storage.googleapis.com/arize-assets/dev-rel/prompt-learning/queries.csv")


dataset_50 = dataset_1000.sample(NUM_SAMPLES) if NUM_SAMPLES > 0 else dataset_1000

# split
train_set = dataset_50.sample(frac=TRAIN_SPLIT_FRACTION, random_state=42)
test_set = dataset_50.drop(train_set.index)

train_set.to_csv("train.csv", index=False)
test_set.to_csv("test.csv", index=False)

"""## Initial System Prompt

Initialize your system prompt. This is the original prompt that will be tested and optimized.
"""

system_prompt = "You are an expert in JSON webpage creation. This is your task: {input}"

"""## Evaluator

Here we initialize our evaluator. This uses LLM as a Judge, or using an LLM to evaluate our outputs.  

We will pass in a set of 10 rules to this LLM. It will evaluate each generated JSON against these 10 rules, checking if all are satisfied.

Accordingly, it will give a correctness label, either correct or incorrect.

Additionally, it will attach an explanation as to why it chose correct or incorrect. These explanations will be used to optimize the prompt.
"""

import re
import json
from phoenix.evals import OpenAIModel, llm_generate
import nest_asyncio
nest_asyncio.apply()

def find_correctness(output):
    """Extract correctness from LLM output"""
    # Look for "correct" or "incorrect" in the response
    pattern = r'"correctness":\s*"?(correct|incorrect)"?'
    match = re.search(pattern, output, re.IGNORECASE)
    if match:
        return match.group(1).lower()
    else:
        return None

def find_explanation(output):
    """Extract explanation from LLM output"""
    # Look for explanation field in JSON
    pattern = r'"explanation":\s*"([^"]*)"'
    match = re.search(pattern, output, re.IGNORECASE)
    if match:
        return match.group(1)
    else:
        return None

def evaluate_output_parser(response: str, row_index: int) -> dict:
    """Parser function for evaluate_output evaluator"""
    correctness = find_correctness(response)
    explanation = find_explanation(response)

    return {
        "correctness": correctness,
        "explanation": explanation
    }

def rule_checker_parser(response: str, row_index: int) -> dict:
    """Parser function for rule_checker evaluator"""
    explanation = find_explanation(response)

    return {
        "rule_violations": explanation
    }

def evaluate_output(dataset, num_rules=NUM_RULES):
    """Evaluator that checks JSON web page correctness using llm_generate"""

    # Create the evaluation template
    with open(f"prompts/evaluator-prompt-{num_rules}.txt", "r") as file:
        evaluation_template = file.read()

    # Create the model
    eval_model = OpenAIModel(
        model="gpt-4o",
        model_kwargs={
            "response_format": {"type": "json_object"},
            "temperature": 0
        }
    )

    # Generate evaluations using llm_generate
    evaluation_results = llm_generate(
        dataframe=dataset,
        template=evaluation_template,
        model=eval_model,
        output_parser=evaluate_output_parser,
        concurrency=20,
        verbose=True
    )

    # Merge the results back into the original dataset
    dataset = dataset.copy()

    for col in ["correctness", "explanation"]:
        if col in evaluation_results.columns:
            dataset[col] = evaluation_results[col]

    return dataset, ["correctness", "explanation"]

def rule_checker(dataset, num_rules=NUM_RULES):
    """Evaluator that checks which rules are broken using llm_generate"""

    # Create the rule checking template
    with open(f"prompts/rule-checker-prompt-{num_rules}.txt", "r") as file:
        rule_check_template = file.read()

    # Create the model
    eval_model = OpenAIModel(
        model="gpt-4o",
        model_kwargs={
            "response_format": {"type": "json_object"},
            "temperature": 0
        }
    )

    # Generate rule checks using llm_generate
    rule_check_results = llm_generate(
        dataframe=dataset,
        template=rule_check_template,
        model=eval_model,
        output_parser=rule_checker_parser,
        concurrency=40,
        verbose=True
    )

    # Merge the results back into the original dataset
    dataset = dataset.copy()
    if "rule_violations" in rule_check_results.columns:
        dataset["rule_violations"] = rule_check_results["rule_violations"]

    return dataset, ["rule_violations"]

from phoenix.evals import llm_generate

def generate_output(dataset, system_prompt):
    output_model = OpenAIModel(
        model="gpt-4.1-2025-04-14",
        model_kwargs={
            "response_format": {"type": "json_object"},
            "temperature": 0
        }
    )
    outputs = llm_generate(
        dataframe=dataset,
        template=system_prompt,
        model=output_model,
        concurrency=40,
        verbose=True
    )
    return outputs["output"]

test_set.head()

"""## Optimization

There are 3 steps to every loop of optimization.

1. Generate outputs with current prompt on test dataset. Evaluate outputs.
2. If outputs are not satisfactory, generate outputs on training set. Evaluate training outputs.
3. Use training outputs and evaluations to generate optimized prompt.

This process repeats until we see good results on the test set. In this case, we measure our results to be satisfactory when all outputs are deemed "correct" by the evaluate_output evaluator we defined above.
"""

from arize_toolkit.extensions.prompt_optimizer import MetaPromptOptimizer

num_rules = NUM_RULES  # Use config value from top of file

from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score

def compute_metric(y_true, y_pred, scorer="accuracy"):
    """
    Compute the requested metric for binary classification.
    y_true and y_pred should be lists or arrays of "correct"/"incorrect" labels.
    scorer: one of "accuracy", "f1", "precision", "recall"
    """
    # Map to binary
    y_true_bin = [1 if y == "correct" else 0 for y in y_true]
    y_pred_bin = [1 if y == "correct" else 0 for y in y_pred]
    if scorer == "accuracy":
        return accuracy_score(y_true_bin, y_pred_bin)
    elif scorer == "f1":
        return f1_score(y_true_bin, y_pred_bin, zero_division=0)
    elif scorer == "precision":
        return precision_score(y_true_bin, y_pred_bin, zero_division=0)
    elif scorer == "recall":
        return recall_score(y_true_bin, y_pred_bin, zero_division=0)
    else:
        raise ValueError(f"Unknown scorer: {scorer}")

def optimize_loop(
    train_set,
    test_set,
    system_prompt,
    evaluators,
    threshold=1,
    loops=5,
    scorer="accuracy",
    num_rules=NUM_RULES
):
    """
    scorer: one of "accuracy", "f1", "precision", "recall"
    threshold: float, threshold for the selected metric
    num_rules: int, number of rules to use for evaluation (determines which prompt files to load)

    Returns:
        dict with keys:
            "train": list of train set scores per run
            "test": list of test set scores per run
            "prompt": list of system prompts used for each test run
            "raw": list of test set DataFrames (deepcopy) for each test run
            "num_rules": number of rules used for this experiment
    """
    import copy

    curr_loop = 1
    train_metrics = []
    test_metrics = []
    prompts = []
    raw_dfs = []

    print(f"🚀 Starting prompt optimization with {loops} iterations (scorer: {scorer}, threshold: {threshold})")
    print()
    
    # Initial test evaluation before optimization
    print(f"📊 Initial evaluation:")
    test_set["output"] = generate_output(test_set, system_prompt)
    
    test_evals_all = evaluate_output(test_set, num_rules)[0]
    test_evals = test_evals_all["correctness"]
    y_true = ["correct"] * len(test_evals)
    y_pred = test_evals
    initial_metric_value = compute_metric(y_true, y_pred, scorer=scorer)
    test_metrics.append(initial_metric_value)
    prompts.append(system_prompt)
    raw_dfs.append(copy.deepcopy(test_set))
    
    print(f"✅ Initial test {scorer}: {initial_metric_value}")
    print('\n')
    
    if initial_metric_value >= threshold:
        print(f"🎉 Initial prompt already meets threshold!")
        result = {
            "train": train_metrics,
            "test": test_metrics,
            "prompt": prompts,
            "raw": raw_dfs,
            "num_rules": num_rules
        }
        return result
    
    while loops > 0:
        print(f"📊 Loop {curr_loop}: Optimizing prompt...")
        
        # 1. Train set evaluation and optimization
        train_outputs = generate_output(train_set, system_prompt)
        train_set["output"] = train_outputs

        train_set["correctness"] = [None] * len(train_set)
        train_set["explanation"] = [None] * len(train_set)
        train_set["rule_violations"] = [None] * len(train_set)

        optimizer = MetaPromptOptimizer(
            prompt=system_prompt,
            model_choice="gpt-4o",
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )

        # Create evaluators with the correct num_rules parameter
        # this is necessary because the evaluators are defined with a default value of NUM_RULES.
        # Your evaluators might be defined differently.
        evaluators_with_rules = []
        for evaluator in evaluators:
            if evaluator.__name__ == 'evaluate_output':
                evaluators_with_rules.append(lambda ds, nr=num_rules: evaluate_output(ds, nr))
            elif evaluator.__name__ == 'rule_checker':
                evaluators_with_rules.append(lambda ds, nr=num_rules: rule_checker(ds, nr))
            else:
                evaluators_with_rules.append(evaluator)
        
        train_set, _ = optimizer.run_evaluators(
            train_set,
            evaluators_with_rules,
            feedback_columns=["correctness", "explanation", "rule_violations"]
        )

        system_prompt = optimizer.optimize(
            train_set,
            "output",
            feedback_columns=["correctness", "explanation", "rule_violations"],
            context_size_k=128000
        )

        # Evaluate train set after optimization
        train_outputs_post = generate_output(train_set, system_prompt)
        train_set_post = train_set.copy()
        train_set_post["output"] = train_outputs_post
        train_evals_post_all = evaluate_output(train_set_post, num_rules)[0]
        train_evals_post = train_evals_post_all["correctness"]
        y_true_train_post = ["correct"] * len(train_evals_post)
        y_pred_train_post = train_evals_post
        train_metric_post_value = compute_metric(y_true_train_post, y_pred_train_post, scorer=scorer)
        train_metrics.append(train_metric_post_value)
        print(f"✅ Train {scorer}: {train_metric_post_value}")

        # 2. Test set evaluation with optimized prompt
        test_set["output"] = generate_output(test_set, system_prompt)
        
        test_evals_all = evaluate_output(test_set, num_rules)[0]
        test_evals = test_evals_all["correctness"]
        y_true = ["correct"] * len(test_evals)
        y_pred = test_evals
        metric_value = compute_metric(y_true, y_pred, scorer=scorer)
        test_metrics.append(metric_value)
        prompts.append(system_prompt)
        raw_dfs.append(copy.deepcopy(test_set))
        
        print(f"✅ Test {scorer}: {metric_value}")
        print("\n")
        
        # 3. Check threshold
        if metric_value >= threshold:
            print(f"🎉 Threshold reached! Stopping optimization.")
            result = {
                "train": train_metrics,
                "test": test_metrics,
                "prompt": prompts,
                "raw": raw_dfs,
                "num_rules": num_rules
            }
            return result

        loops -= 1
        curr_loop += 1

    print(f"🔄 All {curr_loop-1} optimization loops completed.")
    result = {
        "train": train_metrics,
        "test": test_metrics,
        "prompt": prompts,
        "raw": raw_dfs,
        "num_rules": num_rules
    }
    return result

def validate_prompt_files(rule_counts):
    """Validate that all required prompt files exist for the given rule counts."""
    import os
    
    missing_files = []
    for num_rules in rule_counts:
        required_files = [
            f"prompts/evaluator-prompt-{num_rules}.txt",
            f"prompts/rule-checker-prompt-{num_rules}.txt"
        ]
        
        for file_path in required_files:
            if not os.path.exists(file_path):
                missing_files.append(file_path)
    
    if missing_files:
        print("❌ Missing prompt files:")
        for file_path in missing_files:
            print(f"   - {file_path}")
        print("\nAvailable prompt files:")
        for file_path in os.listdir("prompts"):
            print(f"   - prompts/{file_path}")
        raise FileNotFoundError(f"Missing {len(missing_files)} required prompt files")
    
    print("✅ All required prompt files found")

def save_experiment_results(results, filename="experiment_results.json"):
    """Save experiment results to a JSON file."""
    import json
    from datetime import datetime
    
    # Add timestamp to results
    results_with_timestamp = {
        "timestamp": datetime.now().isoformat(),
        "results": results
    }
    
    with open(filename, 'w') as f:
        json.dump(results_with_timestamp, f, indent=2, default=str)
    
    print(f"✅ Results saved to {filename}")

def save_single_experiment_csv(results, filename):
    """
    Save a single experiment's results to CSV format.
    
    Args:
        results: Results from a single optimize_loop run
        filename: Output CSV filename
    """
    import pandas as pd
    from datetime import datetime
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Extract data
    num_rules = results['num_rules']
    train_metrics = results.get('train', [])
    test_metrics = results['test']
    prompts = results['prompt']
    
    # Create DataFrame
    data = []
    for i, (test_metric, prompt) in enumerate(zip(test_metrics, prompts)):
        row = {
            'iteration': i,
            'num_rules': num_rules,
            'test_accuracy': test_metric,
            'prompt': prompt
        }
        
        # Add train metric if available
        if i < len(train_metrics):
            row['train_accuracy'] = train_metrics[i]
        
        data.append(row)
    
    df = pd.DataFrame(data)
    
    # Save to CSV with timestamp
    filename_with_timestamp = f"{filename}_{timestamp}.csv"
    df.to_csv(filename_with_timestamp, index=False)
    print(f"✅ Results saved to {filename_with_timestamp}")
    print(f"   📊 {len(df)} iterations, {num_rules} rules")
    print(f"   📈 Final accuracy: {df['test_accuracy'].iloc[-1]:.3f}")

def save_multi_experiment_csv(results, base_filename="experiment_results"):
    """
    Save multiple experiments' results to separate CSV files.
    
    Args:
        results: Results from run_multi_rule_experiments
        base_filename: Base name for the CSV files
    """
    from datetime import datetime
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    for experiment_name, experiment_results in results.items():
        csv_filename = f"{base_filename}_{experiment_name}_{timestamp}.csv"
        save_single_experiment_csv(experiment_results, csv_filename)



def run_multi_rule_experiments(
    train_set,
    test_set,
    system_prompt,
    rule_counts=[10, 50, 100],
    threshold=0.7,
    loops=5,
    scorer="accuracy"
):
    """
    Run optimization experiments with different numbers of rules.
    
    Args:
        train_set: Training dataset
        test_set: Test dataset
        system_prompt: Initial system prompt
        rule_counts: List of rule counts to test
        threshold: Threshold for stopping optimization
        loops: Number of optimization loops
        scorer: Metric to use for evaluation
        
    Returns:
        dict: Results for each rule count experiment
    """
    all_results = {}
    
    print(f"🚀 Starting multi-rule experiments with rule counts: {rule_counts}")
    print("=" * 80)
    
    # Validate that all required prompt files exist
    validate_prompt_files(rule_counts)
    
    for num_rules in rule_counts:
        print(f"\n📊 Running experiment with {num_rules} rules...")
        print("-" * 60)
        
        # Update global NUM_RULES for this experiment
        global NUM_RULES
        NUM_RULES = num_rules
        
        # Run optimization for this rule count
        evaluators = [evaluate_output, rule_checker]
        results = optimize_loop(
            train_set, test_set, system_prompt, evaluators,
            threshold=threshold,
            loops=loops,
            scorer=scorer,
            num_rules=num_rules
        )
        
        all_results[f"{num_rules}_rules"] = results
        
        print(f"✅ Completed experiment with {num_rules} rules")
        print(f"   Final test {scorer}: {results['test'][-1]:.3f}")
    
    print("\n" + "=" * 80)
    print("🎉 All experiments completed!")
    
    # Print summary
    print("\n📈 Summary of Results:")
    for rule_count, results in all_results.items():
        num_rules = results['num_rules']
        final_test_score = results['test'][-1]
        print(f"   {rule_count}: Test {scorer} = {final_test_score:.3f}")
    
    return all_results

# Run experiments based on configuration
if RUN_MULTI_RULE_EXPERIMENTS:
    print("🚀 Running multi-rule experiments...")
    multi_results = run_multi_rule_experiments(
        train_set, test_set, system_prompt,
        rule_counts=RULE_COUNTS_TO_TEST,
        loops=NUM_OPTIMIZATION_LOOPS
    )
    
    # Save results
    save_experiment_results(multi_results, "multi_rule_experiments.json")
    
    # Save CSV results
    save_multi_experiment_csv(multi_results, "multi_rule_experiments")
    
    print("\n📊 Multi-rule experiment results:")
    print(multi_results)
else:
    print("🚀 Running single experiment...")
    
    # Validate that required prompt files exist
    validate_prompt_files([NUM_RULES])
    
    evaluators = [evaluate_output, rule_checker]
    results = optimize_loop(
        train_set, test_set, system_prompt, evaluators,
        loops=NUM_OPTIMIZATION_LOOPS
    )
    
    # Save results
    save_experiment_results(results, "single_experiment_results.json")
    
    # Save CSV results
    save_single_experiment_csv(results, "single_experiment")
    
    print("\n📊 Single experiment results:")
    print(results)

